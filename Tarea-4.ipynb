{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de admisiones estudiantiles con KERAS.\n",
    "\n",
    "## Mi primera red neuronal ?\n",
    "\n",
    "Información de la UCLA basada en 3 datos:\n",
    "\n",
    "   - GRE Scores (Test)\n",
    "   - GPA Scores (Grades)\n",
    "   - Class rank (1-4)\n",
    "\n",
    "El dataset original se encuentra en: http://www.ats.ucla.edu/ y en este mismo apartado corresponde al file *binary.csv* extraido directaemten del sitio.\n",
    "\n",
    "Debe tener instalado en el ambiente de trabajo Pandas, Keras etc.\n",
    "\n",
    "# 1. Carga y vizualización de datos:\n",
    "\n",
    "Para cargar los datos usamos load the data, usaremos un paquete de datos muy útil llamado Pandas. Puede leer en la documentación de Pandas aquí: https://pandas.pydata.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     admit  gre   gpa  rank\n",
      "0        0  380  3.61     3\n",
      "1        1  660  3.67     3\n",
      "2        1  800  4.00     1\n",
      "3        1  640  3.19     4\n",
      "4        0  520  2.93     4\n",
      "5        1  760  3.00     2\n",
      "6        1  560  2.98     1\n",
      "7        0  400  3.08     2\n",
      "8        1  540  3.39     3\n",
      "9        0  700  3.92     2\n",
      "10       0  800  4.00     4\n",
      "11       0  440  3.22     1\n",
      "12       1  760  4.00     1\n",
      "13       0  700  3.08     2\n",
      "14       1  700  4.00     1\n",
      "15       0  480  3.44     3\n",
      "16       0  780  3.87     4\n",
      "17       0  360  2.56     3\n",
      "18       0  800  3.75     2\n",
      "19       1  540  3.81     1\n",
      "20       0  500  3.17     3\n",
      "21       1  660  3.63     2\n",
      "22       0  600  2.82     4\n",
      "23       0  680  3.19     4\n",
      "24       1  760  3.35     2\n",
      "25       1  800  3.66     1\n",
      "26       1  620  3.61     1\n",
      "27       1  520  3.74     4\n",
      "28       1  780  3.22     2\n",
      "29       0  520  3.29     1\n",
      "..     ...  ...   ...   ...\n",
      "370      1  540  3.77     2\n",
      "371      1  680  3.76     3\n",
      "372      1  680  2.42     1\n",
      "373      1  620  3.37     1\n",
      "374      0  560  3.78     2\n",
      "375      0  560  3.49     4\n",
      "376      0  620  3.63     2\n",
      "377      1  800  4.00     2\n",
      "378      0  640  3.12     3\n",
      "379      0  540  2.70     2\n",
      "380      0  700  3.65     2\n",
      "381      1  540  3.49     2\n",
      "382      0  540  3.51     2\n",
      "383      0  660  4.00     1\n",
      "384      1  480  2.62     2\n",
      "385      0  420  3.02     1\n",
      "386      1  740  3.86     2\n",
      "387      0  580  3.36     2\n",
      "388      0  640  3.17     2\n",
      "389      0  640  3.51     2\n",
      "390      1  800  3.05     2\n",
      "391      1  660  3.88     2\n",
      "392      1  600  3.38     3\n",
      "393      1  620  3.75     2\n",
      "394      1  460  3.99     3\n",
      "395      0  620  4.00     2\n",
      "396      0  560  3.04     3\n",
      "397      0  460  2.63     2\n",
      "398      0  700  3.65     2\n",
      "399      0  600  3.89     3\n",
      "\n",
      "[400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://stats.idre.ucla.edu/stat/data/binary.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualización de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "def plot_points(data):\n",
    "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
    "    y = np.array(data[\"admit\"])\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
    "    plt.xlabel('Test (GRE)')\n",
    "    plt.ylabel('Grades (GPA)')\n",
    "plot_points(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En apariencia GRADES y TEST no parecen tener relación clara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3xcdZ3/8dcJ4VouqUZThhTk0kRuC6xQW9LfilzacPlh3V/7aXVVlGBEEfgJ3pbVYPCnqywq7ANciGRVXBfzkUJggV52VcR0MQqslEJLggoSQltrpwhWqbXz++OcmU6nuUwmmTOTyfv5eOSRmTNn5nznk8n5zPleg1QqhYiICEBVqQsgIiLlQ0lBREQylBRERCRDSUFERDKUFEREJENJQUREMpQURGISBMFzQRB8ptTlEBmJkoJMWUEQfCsIglT085cgCAaCILgjCILDSl22tCAI3hkEwfIgCDZE5XxPqcsklU1JQaa6nwCHAocD7wZOAb5f0hLt7kDgZ8CHS10QmRqUFGSq255KpTakUqkXU6nUw0AHMDcIgoPTOwRBcE4QBA8FQbAlCIKXgyD4cRAEs7NfJPoW/5EgCL4TBMErQRC8EATBJ0c6cBAEZ0evd/Vw+6RSqe+kUqlrU6nUPeN9oyL5UFIQiQRBkAAWAX+JftIOBG4B5gCnA/3AiiAIXp/zEtcCDwMnA/8EfDkIgrcPc6y/A7qBD6dSqa9M5PsQGQ8lBZnqzgiC4NUgCLYBLwJ/A9yYSqX+kN4hlUrdk0qlvp9KpfpSqdRTQCsQAM05r9WVSqW+kUqlfplKpf4ZeAaYn3vAIAg+DnwdeGcqlfr3Ir0vkYJUl7oAIiXWC1wE7AcYcA7w2ewdgiA4ErgOmAu8kfDL1AHAETmv9Yuc+y8CdTnbWqPXaEqlUo9NQPlFJpSuFGSq+2MqlXo2lUqtTaVSbcDzhFVF2e4nbIi+jLAK6WRgE7BPzn7bc+6n2PN/7BHg90BLEATBBJRfZEIpKYjs7nPARUEQnAoQtRscB3wplUqtTKVSTwN/Ivy2X4gngbcDfwt0KDFIuVFSEMmSSqXWE14Z/GO0KQn8FvhgEAQNQRDMBe4E/jiOYzwFvA04D/hmEATD/h8GQfC6IAhODoLg5GjT4dH9wws9vshIlBRE9nQ9cHYQBGelUqmdwGLgaGAN8C3gRuCl8RwglUo9Q5gYzgS+EwTBXsPseiHwP9EPwBei29eN5/giwwm08pqIiKTpSkFERDKUFEREJENJQUREMpQUREQkY7KPaFYruYhIYYYcIzPZkwKDg4OlLkJR1NbWsnnz5lIXo6QUg5DiEFIcJi4GiURi2MdUfSQiIhlKCiIikqGkICIiGUoKIiKSoaQgIiIZSgoiIpKhpCAiIhmxjlMws72AR4EX3f2CnMf2Be4A3gL8Dlji7s/FWT4Rkaku7iuFK4F1wzzWAiTd/Rjga8CXYyuVVIxkMklvby9bt24tdVH2EFfZKu04AGvWrOHqq6/mySefLPqxxirOePf09BT9OLElBTOrB84Hbh9ml3cA345u3wWcZWZaqlDy1tHdTbM7i6ZNY0FXFx3d3aUuUkZ3Rwfe3My0RYvoWrCA7o6OohwnrhjE9X4AmltaOG/5cm6+4ALOffBBmltainassYor3unjnLNjR9E/23FeKdwIfBLYOczjhwEvALj7DuBl4PXxFE0mu2QySefGjQy0t7Nz/nwG2tvp3LCBZDJZ6qKRTCbZ2NlJ+8AA83fupH1ggA2dnRNetrhiENf7gfAKYW1DA6mbboIFC0jddBNrZ81izZo1E36ssYoz3nF+tmNpUzCzC4BN7v6YmZ0xzG5DXRXsMeGdmbUCrQDuTm1t7YSVs5xUV1dX7HvL11hisH79egabmnbbNjhvHps2bWLWrFnFKF7e1q9fT1POHF3zBgfzLlu+cYgrBuN9P2PxwAMPkLpgt+ZHUuefz/IHH+TMM8+c0GONVZzxjvOzHVdDcxNwoZmdB+wHHGxm/+bu78naZwCYCQyYWTVwCLAl94XcvQNIX6umKnWCLE3+NbYYzJgxg0RXFwPz52e2JXp6qFu6tORxnDFjBl2JBPMHBjLbehIJltbV5VW2fOMQVwzG+37G4oILLuCWBx8ktWBBZlvwwAOcd/75ZfF3jSveNbffzpas49SsWEHdJZcUfJyST4jn7n/v7vXu/iZgKfDDnIQAcB9wUXR7UbSPpsaWvNTU1NBSV0d9WxtVq1ZR39ZGy4wZ1NTUlLpo1NTUUNfSQlt9Pauqqmirr2dGS8uEly2uGMT1fgBOPPFETujrI7jiCli5kuCKKzihv58TTzxxwo81VnHFO5VKcer993PExRdTtXw5R1x8MW+5/35SqeKcHoNivfBwouqjj7v7BWZ2HfCou99nZvsB3wFOIbxCWOruvxrl5VKaOrtyFRKDZDJJf38/DQ0NZZEQshVatrHGIa4YxBnrNWvWsHz5cs4777yySAjZih2H3t5epi1axKk7d/I0cDzw86oqti1bxuzZswt6zehKYciOPLEnhQmmpFDBFIOQ4hCaqnHYunUrXQsW0J5VXddWX8/SlSsLTkIjJQWNaBYRKWNxVtdBBay8JiJS6Ra2tpJcvJhNmzaxtK6uqNV1SgoiIpPA9OnTmTVrVtGr0FR9JCIiGUoKIhKrcp6fSpQURMpaXJOgxSXOOZOkMEoKImUqzknQ4hDnnElSOCUFkTJUzhP8Faqvr2/IOZP6+/tLVCIZipKCSBnq6+sbchK0yXwCbWxsZHXOnDs9iQQNDQ0lKpEMRUlBpAw1NjaSWL16t22Jnp5JfQKNexCWFEbjFETKUHqytc62NgbnzSPR01M2E/yNR3oQVn9/P0vLcH4q0dxHZWuqzvOSTTEI2xY2bdpEXZFHsU4G+jxMXAxGmvtIVwoiZSyuUawiaWpTEBGRDCUFERHJUFKQilLIFAqadkFkl1jaFKJV1R4G9o2OeZe7X5uzz+HAt4EaYC/g0+7+YBzlk8rQ0d1N58aNDDY1kejqoqWujtaFCyf8OSKVLK4rhdeAM939JOBkoNnM5uTs8xnA3f0UwnWcvx5T2aQCFDICuBJHDYuMVyxJwd1T7v5qdHfv6Ce3L2wKODi6fQhQmX1NpSgKGQFciaOGRcYrti6pZrYX8BhwDHCLu/fm7PI5YJWZXQ5MA84e5nVagVYAd6e2trZoZS6l6urqin1v+RpLDE4//XRm3nYbz8+fn9lW/8gjzL30UqZPnz5hzykFfRZCikM8MYh98JqZ1QD3AJe7+9qs7VcBgbt/xczmAp3ACe6+c4SX0+C1CjbWGHR0d9O5YcNuI4DzalMY43Pips9CSHGo0MFr7r7VzB4CmoG1WQ+1RNtw90eixulaYFPcZZTJqXXhQhYnk/T399OwdGleI4ALeY5IJYulTcHM3hBdIWBm+xNWDa3P2e03wFnRPscC+wG/jaN8UjmmT5/O7Nmzx3RyL+Q5IpUqriuFQ4FvR+0KVYS9jO43s+uAR939PuBq4Btm9jHCRuf3u/uknphJRGSiJJNJ1q9fz4wiT4yoCfHKlOpPFYM0xSE0leOw23ia1avHPZ5mpDYFjWgWESljcY+nUVIQESljfX19vDh37m7bXjz99KKNp1FSEKkwmsupstTV1XHw3Xfvtu3gZct44xvfWJTjKSmIVJCO7m6a3Vk0bRoLurro6O4udZFknDZu3MiSe+9l5sUXU7V8OTMvvpgl993Hpk3F6a2vpCASk2J/g9dcTpWpsbGR5w45hNT27aSeeorU9u38+pBDirZet1ZeE4lBd0cHGzs7aRocpCuRoK6lhYWtrRN6jJHmcpo9e/aEHkvik0qlePSCC9jy1a8CMABsu+oqitVzVFcKIkWWTCbZ2NlJ+8AA83fupH1ggA2dnRP+Db6xsZHE6tW7bUv09BTtG6XEo6+vj63Nzbtt29rcrIZmkcmqr6+PppzxNPMGByf8n7qmpoaWujrq29qoWrWK+rY2Woo80EmKL+5kr+ojkSJrbGykK5Fg/sBAZltPIsHSIvxTay6nypNO9t/4zGfY8L/+FzN+8hNaEomi/W11pSBSZDU1NdS1tNBWX8+qqira6uuZ0dJStH9qzeVUoaKGZrZvL+phdKUgEoOFra0kFy+mv7+fpQ0NOmFL3tK9ygavvx4IVx/rbGtjcTJZlHU/dKUgY6KBUYXTN/h4VcpnNe4VApUUJG8aGFW5KuUEmlZJn9W4G5qVFCQvGhhVuSrpBAqV91mNu1eZkoLkRYvcTx5j+dZfaSdQqMzPauvChaxYsoT/2ntvVi5dWtQlY2NpaI6W1nwY2Dc65l3ufu0Q+xnwOcJFdp5w93fHUT4ZXWNjI4muLgayFrlP9PTQsHRpCUsluXabd7+ra9R59ytxFHSlflanT5/OrFmzir6mRFxXCq8BZ7r7ScDJQLOZzcnewcxmAX8PNLn78cD/jalskgcNjCp/hXzrr8RR0Pqsjk8sVwrRspqvRnf3jn5yJ+74IHCLuyej5xRnCkApmAZGlbdCvvWnT6CdbW0MzptHoqenIk6g+qwWLrblOKP1mR8DjiE8+X8q5/FuoA9oAvYCPufuK4Z4nVagFcDd37K9yAM5SqW6upodO3aUuhgTYsuWLTz99NMcf/zxY+pXXUkxgMLisGXLFp555hne/OY3j/qcZDLJW2+7jefb2jLbDm9v52eXXjrqc7ds2cK6des47rjjitL3fSJU2uehEBMVg3322QeGWY4z9jWazawGuAe43N3XZm2/H/gzYEA98BPgBHcfqbVMazSXufGsLVspMYDC4lDwczZs2O1bfzEbJeNUSZ+HQk1UDMpqjeboJP8Q0Jzz0ABwr7v/2d1/DTwDzIq5eDKBKrFnSyEKiUOhsUv3Ulm2bVvRe6nEKZlM0tPTUzHjKMpZLEnBzN4QXSFgZvsDZwPrc3brBt4e7VMLNAC/iqN8UhyV2DWwEIXEYTyxq7SR0+lxFOfs2FER4yjKXVxXCocCPzKzNcDPgf909/vN7DozuzDaZyXwOzN7GvgR8Al3/11M5ZMiqMSeLYUoJA6KXUhXm/GLvU1hgqlNocyNp467UmIAhcWho7ubbwwOZqZL/mAiUTHVQfnq7e1l0bRp7Mwac1C1ahXLtm0ryjiKZDJJX18fjY2NZXmlFUebgmZJlaJS18BQwXGIabrkchXnQLQ4lkwdj2Qyyfr165lR5C7DulIoU5X0LblQUzkGyWSSZncG2tsz2+rb2lixZEnZdhktljh6VCWTSby5mfashZDa6utZsmJFWcR7PL34hqIrBZFJphKnnyhU+ipr06ZN1BXpanOkJVNLHe/sdhWAgfnztZ6CyFSjhubdTZ8+naampqJVmzQ2NrI6/Pac0ZNIlEW84+7FN+KVgplVAxcC5wMnATXAVuAJYDnQ7e5Te4ihSBFU6vQT5SqzZGpnJ/MGB+lJJIq6ZOpYxD3B37BtCmb2IeAfgHXAj6PfrwAHAccCb4t+f9Hdby1K6UanNoUKFlcMyrnHSTJdbVJXV3Zli1scn4dkujNAmS2ZOtHtKoW2KTQAs919wxCP3QN80cwOBa4uuGQiJTbWqabjFtd0yRJKD/wrN3G0q6QNmxTcPZ+T/UZ3//gElkckNnE34ImMR1xfEArqfWRmJwIXAe8GEqPsLlKW1MNHZE95JwUzewNhEriIsNH5J8CVRSqXSNFV6gpdIuMxWu+jvQl7H70fWAA8C9wJHAGYFsKRyUw9fET2NNqVwkZgJ/At4Fp3fxzAzD5S5HKJxELTcIjsbrTBa2sIxya8FTjNzNT6JhWn0qaaFhmPEZOCu58BHA2sAj4ObDCz/wCmEa6zLCIiFWTUaS7c/Xl3/7y7zwLOAl4irFJ6wsyuL3YBRUQkPmOa+8jde9y9FZgBXA6cWJRSiYhISYzW+ygAPgicADzu7t8CcPc/EfZCujOfg5jZfsDDwL7RMe9y92uH2XcR8H3gNHd/NL+3ISIiE2G0K4UbgHbCK4N/NLP2UfYfzmvAme5+EnAy0Gxmc3J3MrODgCuA3gKPIyIi4zBaUjDgbe5uhO0J7y7kIO6ecvdXo7t7Rz9DzcT3eeB64E+FHEdERMZntHEKh7h7H4C7P21mryv0QGa2F/AYcAxwi7v35jx+CjDT3e83s2HnUzKzVqA1KhO1tbWFFqmsVVdXV+x7y5diEFIcQopDPDEYcTlOM3sF+Ct2TbH6OHBK1n3c/VdjOaCZ1RDOsnq5u6+NtlUBPwTe7+7PmdlDwMfzaFPQ1NkVTDEIjTUO5TwV+Hjo8zBxMRhp6uzRqo+mEU5tkf45GPhl1v0xL/3j7luBh4DmrM0HETZmP2RmzwFzgPvM7NSxvr5IHJLJJL29vWzdurXURdlNR3c3ze4smjaNBV1ddHR3l7pIMsmMWH3k7hOyXGc0md6f3X2rme0PnA18Oes4LwO1Wfs/RH5XCiKxK9c1GDQVuEyEuNZoPhT4kZmtAX4O/GfUdnCdmV0YUxlExi37xLtz/nwG2tvp3LCBZDJZ6qLFvpavVKbRximcBpzr7tdF99cRjjXI7JLPt3l3X0PYFpG7vW2Y/c8Y7TVFSqGc12DQVOAyEUa7UvgEsDbrfoJwMNsHgQ7gU0Uql0hZamxsJLF69W7bEj09NDQ0lKhEu6SnAq9va6Nq1Srq29o0FbiM2WhdUmcTLqqTttPdfwBgZj3A+mIVTKQclfsaDHFOBV6pvZymutGSwuvZfSDZGVm3/0xW47DIVFHuazDEsfh8d0cHGzs7aRocpCuRoK6lhYWtrUU95lgVkrSU6EavPtoMvDl9x92fyHrsWOB3xSiUSLkr5zUYit1dNplMsrGzk/aBAebv3En7wAAbOjvLorE9rbujA29uZtqiRXQtWEB3R0dRnlOJRksK9wA3RhPaZUTdSr8C3F2sgokUolzHD8QljhNbX18fTTmDRucNDpZNL6dCktZkSHRxGS0ptAGvA35pZv9qZl80s28SDlyrBYac6VSkFKb6N724TmyNjY2sDkfEZvQkEmXR2A6FJa1yT3RxGm3ltVeBJsKT//7AadHvzwFN7v5KsQsoko/xnBAr5eoirhNbTU0NdS0ttNXXs6qqirb6ema0tBS9Ubunpyevv1EhSavcE12cRmtoxt23A7dHPyJlaaQT4kiNrpOhwTRfjY2NdCUSzB8YyGzrSSRYWoQT28LWVpKLF9Pf38/ShoaiJoSx/o0ySauzk3mDg/QkEqMmrUKeU6mGnRDPzK4AbnP314Z7spntC3zI3f+5SOUbjSbEq2BjicHWrVvpWrCA9qwTYlt9PUtXrhz2HzuZTOLNzXs8Z8mKFWU1LcRY4tDd0cGGnBPbZE1yML6/UTLdQ2wMSauQ58QpjgnxRrpSmAE8a2YPAj8GngFeIZy8roGwe+q5wB3jLqHIOBXyTa/Qq4tCxdHdMc5v8HEYz9+okK65cXTnLXfDtim4+zWEU1P0Ay3AcsLRzQ8CFxMOXDvF3T8TQzlFRrWwtZUlK1awbdkylq5cOeo35DjrkeNsBC/n7rJjpbr++I02S+pmwiU5b4inOCLjM5ZvenHVI2c3ggPMHxigrbOT5OLFZVVNVY5U1x+/ERfZmQTUplDB4opBseuRe3t7mbZoEfN37sxsW1VVxbZly/JKYPoshH+jTZs2UVdXN6UTQjkssiMVrFK6Yo5XsatbxlMFMpaumJVs+vTpNDU1TemEENdnQUlhiprqA73iVGi//vTfaMc55+hvNMWlV9Q7Z8eOoq+oF0v1UTRNxsOEazFUA3e5+7U5+1wFXALsAH4LXOzuz4/y0qo+KkAldsUcj7gmQRtLNdVk+RvFaapWoyWTSZrdMyvqAdS3tbFiyZKCPwvjrj4yszeY2YHR7b3M7ANm9j4zy/dK4zXgTHc/CTgZaDazOTn7/A9wqrv/FXAXcH2ery1jpCH9u5RrryD9jSQt7hX18j2p3w/Mim5/Afg4cBXhpHijcvdUNGUGwN7RTypnnx+5+7bo7k+B+jzLJmOkbn6hcp4ETX8jSYt7YadRp7mINAC/iG6/BzgdeBV4CvhYPi9gZnsBjwHHALe4e+8Iu6fHRQz1Oq1AK4C7U1tbmUs6VFdXF+291dbWctQVV9B+883MHRjgkfp6jv7oRznmmGOKcrxCFTMGAOvXrx/y2/imTZuYNWvWMM+Kx2T5G8Wp2J+HclVbW8sVRx3Fze3tDMydS/0jj/DRo48u2mchrzYFM9sMHEaYHL7n7sdHVUcvu/tBYzmgmdUQTsl9ubuvHeLx9wAfBd420hQbEbUpjMNUGdI/nPTUGFcODPAUcALwtVGmxoibumLuMlXbFNIm8rNQ6DQX2ZYDTrgS2/eibccBL461MO6+1cweAprZff1nzOxs4B/ILyHIOE31If01NTU8dcYZHLX//rzyzndy0D33cPYf/5jXP1xcjdPTp09n1qxZU/pkKKG4Pgv5tilcAjwAdAL/GG2rJZxCe1RRQ3VNdHt/4Gxy1nc2s1OA24AL3X1TnuUSKVgymaT3qKN4uaODneeey8sdHfQeeeSobQrqziuVLK8rhehbe0dUZVQHvOTuD43hOIcC347aFarCl/T7zew64FF3vw/4J+BA4PtmBvAbd79wDMcQGZORenUMdwWlKSuk0uWVFKJv+V8HFgF/BqaZ2YXA7HwmxHP3NYST6+Vub8u6fXa+hS4VLepdWRobG0l0dTEwf35mW6Knh4alS4d9Ttwzq4rELd/qo1uBl4EjgO3RtkeAJcUoVDlSlUHlqampoaWujvq2NqpWraK+rY2WGTNGTPjqKiqVLt+kcBZwhbu/RDS+wN1/C7yxWAUrJ+Xcn13Gp3XhQrqam/n8o4/i555L68KFI+5fiqUoReKUb++jlwkbll9KbzCzw7PvVzJVGVSu7KUeV+S5HGelLWQjki3fK4XbgWVm9nagyszmAt8mrFaqeKoyqEzjuQKspIVsRLLlmxS+TDhO4RbCKSr+FbgXuKlI5SorqjKoTJpfSGRP+XZJTQE3Rj9TkqoMKk9jYyNdiQTzs2Yi7UkkWKorQJnChk0KZnZmPi/g7j+cuOKUt6k+ArjSaKlHkT2NdKXQmXP/MMKeR78jnO4iAAaAo4pTNJHi0xWgyO6GTQrufmT6tpldQ5gIPuvu28zsAOA6wgQhMqnpClBkl3wbmj8GfDq93kH0++8J11QQEZEKkW9S+AOQ+1XqNGDbEPuKiMgkle/gtc8CK8zsP4AXgJnABcBlxSqYSFw0p5XILnldKbj7d4C3AuuAgwmnvZ4TbReZtDSnlcju8r1SwN2fBp4uYllkHMr52265lk3TYIvsKe+kEE2V/TbCOZAyy7i5+/uKUC4Zg+z5e7rynL8nLh3d3XRu3MhgUxOJri5a6upGnXQuLprTSmRPeVUfmdm1hKuiVQGLCbuiLgC2Fq9oko9ynsE1mUzSuXEjA+3t7Jw/n4H2djo3bCiLsoHmtBIZSr5XChcD57j7WjP7gLt/zMzuBEZdYAfAzPYDHgb2jY55l7tfm7PPvsAdwFsIk84Sd38uz/JNWeX8bbeQlc3ipBHNInvKt0tqjbuvjW5vN7O93f1nhNVJ+XgNONPdTwJOBprNbE7OPi1A0t2PAb5GOAmfjKKcv+02NjaSWL16t22Jnp6yKFvawtZWlqxYwbZly1i6cmXZVLuJlEq+SeGXZnZ8dHst8GEzey+QVz2Au6fc/dXo7t7RTypnt3cQTscNcBdwlpkFyIjinsE1mUzS29vL1q2j1xwWsrJZKRQyDfZY4iAymeRbffQZwmkuAD4N/DtwIPCRfA9kZnsBjwHHALe4e2/OLocRjoHA3XeY2cvRMTfnvE4r0BrtR21tbb5FmFSqq6vzfm+XXHMNWy69lHXr1nHZcccVrefMnTfdxPO33MKcF17grpkzOeKyy3jXlVeO+JxrLrmES7dsYd26dRx32WVjKttYYhCnm+68k1uef54X5sxh5l13cdkRR3Dlu95VtOOVaxzipjjEE4Mglcr9wr47M6sCzgB63H37iDvnwcxqgHuAy7OqpDCzp4AF7j4Q3f8lMNvdR5pfKTWYU59eKWpra9m8efPoO8YkmUzizc2Z7psAbfX1LFmxomhJqNxiAGEcmt0ZaG/PbKtva2PFkiVTKg6loDhMXAwSYZXzkDUxo1YfuftO4N6JSAjR620FHgKacx4aIBwpjZlVA4cAWybimFNBsasztCBNaKTGc5FKkG+bwsNDNAznzczeEF0hYGb7A2cTjorOdh9wUXR7EfDDaHEfGUUco3LLuUE7TpOh8VxkPPJtU3geWG5m9xLW+2dO1u7elsfzDwW+HbUrVIVP8/vN7DrgUXe/j3D9hu+Y2bOEVwhLx/A+pqy4RuWq+2Yo3Xje2dbG4Lx5JHp6yrLxXKRQo7YpAJjZN4d7zN0/MKElGpsp36bQ29vLtEWLmL9zZ2bbqqoqti1bVpSxAMlkkv7+fhpiWJCmnOuQFYf4KQ7xtCnku0ZzKU/8MoK41xnWgjQhxUEq1ahJIRqo9ufo9jx2b4f4b3ffUazCyehUrSMiE2nEpGBmHwZOB94bbVrFriU4DwA+yZ5rOUvMyn2d4XKdJVVE9jRa76P3ATdk3X/N3We6+0zgLOCSopWsiCpxNGoho3Lj0NHdTbM7i6ZNY0FXFx3d3aUukoiMYLSkcKS7P5F1P3s9hSeAoya+SMWlRVXiU+6zpIrInkZLCgea2bT0HXfPHrVzADBtz6eUr3KeZroSaaCXyOQzWlJYC8wf5rFm4KmJLU5xaVRuvDTQS2TyGa330Y3A180sBdzn7jujuZDeAdwMXFXsAk6kuLtvTnUa6CUy+YyYFNz9e2Z2GPBvwD5mtplwOc7XgOvc/c4Yyjhh1H0zfq0LF7I4PdBr6VLFWqTM5Tui+WBgLmFC+B3wiLu/XOSy5aOgEc1xjkYtlEZvKgZpikNIcSivEc2/B1aOuyRlQqNRRUSGlu8sqSIiMgUoKYiISIaSgoiIZCgpiIiMQ6VNm5PvIjvjYmYzgTuAGcBOoMPdb8rZ5xDCrq+HR+W6wd2HXcdBRKTUOrq76dy4kcGmJhJdXbTU1dG6cGGpizUucV0p7ACudvdjgTnAZZqM8OgAAA05SURBVGZ2XM4+lwFPu/tJwBnAV8xsn5jKJyIyJpU6t1csScHdX3L3x6PbrwDrgMNydksBB5lZABxIuCSn1moQkbJUqXN7xVJ9lM3M3gScAvTmPHQzcB8wCBwELHH3nTn7YGatQCuAu1NbW1vU8pZKdXV1xb63fCkGIcUhVG5xOP3005l52208P3/X9HD1jzzC3EsvndD10bPFEYO8RjRPFDM7EPgx8AV3vzvnsUVAE+F8SkcD/wmcFA2cG86UX6O5kikGIcUhVI5x6OjupnPDht3m9ipmm0LZjGieCGa2N7AM+G5uQoh8APiSu6eAZ83s18CbgZ/FVUYRkbGoxLm9YmlTiNoJOoF17v7VYXb7DeFqbphZHdAI/CqO8omIFKpcVz0sVFxXCk2E6zw/aWa/iLZdQ9j9FHe/Ffg88C0ze5LwsuZT7l5e14oiIhUulqTg7j0MU3+Vtc8gwy/oIyIiMdCIZhERyVBSEBGRDCUFERHJUFIQEZEMJQUREclQUhARkQwlBRERyVBSEBGRDCUFERHJUFIQEZEMJQUREclQUhARkQwlBRERyVBSEBGRDCUFERHJiGU9BTObCdwBzAB2Ah3uftMQ+50B3AjsDWx297fFUT4REQnFdaWwA7ja3Y8F5gCXmdlx2TuYWQ3wdeBCdz8eWBxT2UREJBJLUnD3l9z98ej2K8A64LCc3d4N3O3uv4n22xRH2UREZJe41mjOMLM3AacAvTkPNQB7m9lDwEHATe5+R7ylExGZ2oJUKhXbwczsQODHwBfc/e6cx24GTgXOAvYHHgHOd/e+nP1agVYAd3/L9u3b4yh67Kqrq9mxY0epi1FSikFIcQgpDhMXg3322QcgGPIY4371PJnZ3sAy4Lu5CSEyQNi4/AfgD2b2MHASsFtScPcOoCO6m9q8eXMRS106tbW1VOp7y5diEFIcQorDxMUgkUgM+1hcvY8CoBNY5+5fHWa3e4Gbzawa2Ad4K/C1OMonIiKhuK4UmoD3Ak+a2S+ibdcAhwO4+63uvs7MVgBrCLut3u7ua2MqnxRRMpmkr6+PxsZGampqSl0cERlBrG0KRZAaHBwsdRmKolIulbs7OtjY2UnT4CCrEwnqWlpY2Nqa13MrJQbjpTiEFIcJrz4ask1BI5qlaJLJJBs7O2kfGGD+zp20DwywobOTZDJZ6qKJyDCUFKRo+vr6aMq5kps3OEh/f3+JSiQio1FSkKJpbGxkdU4vh55EgoaGhhKVSERGo6QgRVNTU0NdSwtt9fWsqqqirb6eGS0tamwWKWOxj2iWqWVhayvJxYvp7+9naUODEoJImVNSkKKbPn06s2fPLnUxRCQPqj4SEZEMJQUREclQUhARkQwlBRERyVBSEBGRDCUFERHJUFIQEZEMJQUREclQUhCRWCWTSXp7e9m6dWupiyJDUFIQkdh0dHfT7M6iadNY0NVFR3d3qYskOeJajnMmcAcwg3BVtQ53v2mYfU8Dfgoscfe74iifiBRfMpmkc+NGBtrbARiYP5/OtjYWJ5NMnz69xKWTtLiuFHYAV7v7scAc4DIzOy53JzPbC/gysDKmcolITPr6+hhsatpt2+C8eVpfo8zEkhTc/SV3fzy6/QqwDjhsiF0vB5YBm+Iol4jEp7GxkcTq1bttS/T0aH2NMhP7LKlm9ibgFKA3Z/thwDuBM4HTRnh+K9AK4O7U1tYWraylVF1dXbHvLV+KQahS4lBbW8sVRx3Fze3tDMydS/0jj/DRo4/mmGOOyev5lRKH8YgjBkEqlSrqAbKZ2YHAj4EvuPvdOY99H/iKu//UzL4F3J9Hm0JqMGe5x0qhRcoVg7RKi0MymaS/v5+GMa6vUWlxKMRExSARrogYDPVYbL2PzGxvwqqh7+YmhMipwPfM7DlgEfB1M1sYV/lEJB7p9TW04FJ5iqv3UQB0Auvc/atD7ePuR2bt/y3CKwX1VxMRiVFcbQpNwHuBJ83sF9G2a4DDAdz91pjKISIiI4glKbh7D8PUXw2z//uLVxoRERmORjSLiEiGkoKIiGQoKYiISIaSgoiIZMQ6eK0IJnXhRURKqLSD14okqNQfM3us1GUo9Y9ioDgoDkWNwZAme1IQEZEJpKQgIiIZSgrlq6PUBSgDikFIcQgpDjHEYLI3NIuIyATSlYKIiGQoKYiISEbsK6/JLtGa1I8CL7r7BWZ2JPA94HXA48B73X27me0L3AG8BfgdsMTdnytRsSdUtH7GK8BfgB3ufqqZvQ7oAt4EPAeYuyejKdhvAs4DtgHvTy/zOpmZWQ1wO3AC4dibi4FnmFoxaCR8v2lHAW2En/spEwcAM/sYcAnhZ+FJ4APAocR0btCVQmldSbheddqXga+5+ywgCbRE21uApLsfA3wt2q+SvN3dT3b3U6P7nwZ+EMXhB9F9gHOBWdFPK/AvsZe0OG4CVrj7m4GTCD8TUyoG7v5M9Bk4mfAEtw24hykWh2hZ4iuAU939BGAvYCkxnhuUFErEzOqB8wm/IaYXIjoTSC9B+m0gvfLcO6L7RI+fFe1fqbLfb24c7nD3lLv/FKgxs0NLUcCJYmYHA39DuAgV7r7d3bcyhWIwhLOAX7r780zNOFQD+5tZNXAA8BIxnhuUFErnRuCTwM7o/uuBre6+I7o/ABwW3T4MeAEgevzlaP9KkAJWmdljZtYabatz95cAot9vjLZn4hDJjtFkdRTwW+CbZvY/Zna7mU1jasUg11Lgzuj2lIqDu78I3AD8hjAZvAw8RoznBiWFEjCzC4BN7v5Y1uahsnsqj8cmuyZ3/2vC6oDLzOxvRti3EuNQDfw18C/ufgrwB3ZVkQylEmOQYWb7ABcC3x9l14qMg5lNJ/z2fySQAKYR/m/kKtq5QUmhNJqAC6NG1u8RXhreSHgJnG78rwcGo9sDwEyA6PFDgC1xFrhY3H0w+r2JsA55NrAxXRUQ/d4U7Z6JQyQ7RpPVADDg7r3R/bsIk8RUikG2c4HH3X1jdH+qxeFs4Nfu/lt3/zNwN3A6MZ4blBRKwN3/3t3r3f1NhJfKP3T3vwN+BCyKdrsIuDe6fV90n+jxH7p7JXwrmmZmB6VvA/OBtez+fnPj8D4zC8xsDvByumphsnL3DcALUe8bCOvTn2YKxSDHu9hVdQRTLw6/AeaY2QFR20D68xDbuUFJobx8CrjKzJ4lrBfsjLZ3Aq+Ptl/FyNULk0kd0GNmTwA/Ax5w9xXAl4BzzKwfOCe6D/Ag8CvgWeAbwEfiL3JRXA5818zWACcDX2TqxQAzO4Dwvd6dtXlKxSG6YryLsNvpk4Tn6A5iPDdomgsREcnQlYKIiGQoKYiISIaSgoiIZCgpiIhIhpKCiIhkaJZUkSKI+pj/DLjI3Z+O6ZifBA5w98/FcTypTEoKUrHM7NWsuwcArxFO0Q3wIXf/boGv+1PgZnf/txF2W0Q4JXomIZjZscB1wNuBfYANwAPA9e7+kpk1E/a/30Y4VcEA8P/S5TSz/YA/Zj2edo27/zNwC9BvZje5e7KQ9yaipCAVy90PTN+OphS5xN3/K6bDXwp8Pev4xwI/JZzi+Up3HzSzGcB7gbnsGrD1K3c/JrrSeAfwfTP7b3f/ddZrN7r7QO4B3f0PZvYD4O+Am4vyrqTiKSnIlBUtcnQN4SImBwMrgcvcfWs07UYn4dQbVYSL3jQDnwFOA243s1uB29z96pzXPYBwOuwlWZs/D6xy98yI02iKi38aqmzRVAXdZvZHwsV3fj3UfkN4iPAqRUlBCqKkIFPZJwhP+vMIJxG7lXChkg8QrnxVTTg18Z+BU4Dt7n61mTUxcvXRscDv3X1z1razCa8e8mJmVcD/AQ4CfjmG97SOcKEekYIoKchU9iHgPemZWs2sHXjKzC4mTARvAI5297XAz8fwujWES4wSve5ehLNXbsja9nHCq45q4Jvufnn00JFmthXYn3DVrY8M0VD9lJlltym8w91/HN1+JTq+SEGUFGRKiursZwIP5pxgq9g14dgM4C4zO5BwHdzPuvtf9nixPSUJv+ED4O5/MbPfE66zm952A3CDmd0AHJj13F9HbQr7AV8lnFY9d6nJ44dqU4gcBGzNo4wiQ1JSkCnJ3VNm9iLwtzmLHWVrA9rM7CjC9oangO8y+iIm64CDzKw2qwrpB8Dfsvu00COV709mdhXwrJk1R7PH5uNY4Ik89xXZgwavyVR2K/AlM0svUvJGM/vf0e2zzey4qG7/98AOdnVn3Ui4jOaQ3P2PhA2+2avIfRZYYGZfMrNE+nhA456vkHmdPxEuvnTtGN7T24DlY9hfZDdKCjKVXQ/8F/BDM3sF+G/CVc8gbGC+l7COfi3h+AGPHvsa4QIvSTO7fpjXvo2wuykA7v4UYdfTWcCTUXXSw4TrAVw3Qhk7gGPN7Jysbc+Y2atZP1+GzEJFZwMjjZ8QGZHWUxApgqjNohd4f4wjmj8BHOTubXEcTyqTkoKIiGSo+khERDKUFEREJENJQUREMpQUREQkQ0lBREQylBRERCRDSUFERDL+P95UNMBqM3PpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO29fXxcVbXw/522vJS2kmIkadpaCi2RF+WloK0FpbRNU+VixXa14FWhpbnci1JE0Hv90WDh0SsiUPwBjwZzi6jULNsSuAh9AakQKFWL8taGRDGVmLYBMymlgNhmnj/OmTAJyWTOZM6ZMzPr+/nkk5k9e/bZa2bOWWevvV4isVgMwzAMwwAYku0JGIZhGOHBlIJhGIbRjSkFwzAMoxtTCoZhGEY3phQMwzCMbkwpGIZhGN2YUjCMgIhEIi2RSOTabM/DMJJhSsEoWCKRyN2RSCTm/h2MRCKtkUjknkgkMjbbcwOIRCLDIpHIdyKRyB8ikci+SCTyWiQS2RCJRD6W7bkZ+YspBaPQeQIYA3wQuAg4DfhlVmf0LocB04CbganAOcCrwCORSOS4LM7LyGNMKRiFzjuxWGx3LBb7WywWexyoAaZFIpH3xTtEIpHZkUhkcyQS6YhEInsjkchvIpHIRxMHcVcb/xGJRH7q3tW/EolEvp7swJFIZJY73tf6ej0Wi+2PxWIzYrHYz2Kx2IuxWOwF4EvAQeBTg5bcMPrAlIJhuEQikTJgPs5F92DCSyOBO3Du1j8ONAPrI5HI+3sNcR3wOHAqcBNwYyQSmdHPsT4P1AP/HovFbvYwzeHAIcBrHt5jGCkTsdxHRqESiUTuBv4VeBvnBmm4+9LNsVjs6iTvGwL8HfhyLBb7udsWA/7/WCx2RUK/RuC+WCz2X+7zFuDH7vGWA/Njsdgmj3P+MTALODkWi73h5b2GkQrDsj0Bw8gyW3FMMocDAszGuWB3E4lEJgLX49j3j8ZRIEcAE3qN9cdez/8GlPRqq3LHmB6LxbZ5mWgkEvkuMA841xSC4RdmPjIKnbdisdifYrHYC7FYrBrYiWMqSuRBnI3oy3FMSKcC7cChvfq90+t5jPeeY1uA14ElkUgkksoEIw4/AJYAM2Ox2HOpvM8w0sGUgmH05FvAlyKRyBkA7r7BicB3Y7HYhlgsth3H/HN0muM/D8wALgBqBlIMkUhkKPA/wALgnFgs9myaxzWMlDClYBgJxGKxRpyVwX+7TVEcN9ClkUjk+EgkMg1YDbw1iGO8CHwSx4NolbtH8R4ikcgw4BfA+Timrb9HIpFS929kusc3jGSYUjCM9/I9YFYkEpkZi8W6cO7SjwOeA+4GVgK7BnOAWCz2Eo5iOBf4qbsi6M04HG+oo3C8mnYl/PW7EW4Yg8G8jwzDMIxubKVgGIZhdGNKwTAMw+jGlIJhGIbRjSkFwzAMo5tcj2i2XXLDMIz06DNGJteVAm1tbYN6f3FxMa+9Vli5xQpNZpM3vzF5vVNWVtbva2Y+MgzDMLoxpWAYhmF0Y0rBMAzD6MaUgmEYhtGNKQXDMAyjG1MKhmEYRjemFAzDMIxuTCkYhmEY3QQavCYiQ4HfA39T1fN6vXYYcA8wBaco+kJVbQlyfkb4iUajNDU1UV5eTlFRUc6NH7b55MP4jY2NlJaWhuL78orXzycIeYNeKSwDdvTz2hIgqqqTgFuBGwOblZET1NfUoJWVjJg/n7o5c6ivqcno+DX19VSqMn/ECObU1VFTX5/R8b3it7y5/nnG539g9mxf5u83Xj+feP/ZBw74+vsMTCmIyDjg08CP++nyGeAn7uM1wEwRSamwuZH/RKNR9tTWsqK1lYquLla0trK7tpZoNJqx8Wv37KF1xQq6KipoXbGC2t27MzZ+OvPxW94wfp7RaJStW7fS2dmZ1fn7jdfPJ8jfZ5Dmo5XA14FR/bw+FngFQFUPiMhe4P1AjyQfIlIFVLn9KC4uHtSkhg0bNugxco1clLmxsZHpvfJcndXWRnt7O5MnT0763lTkbWxspG369B5tbWedldL4fhCEvOmOnwrpfJ63rV7NHTt38srUqYxfs4bLJ0xg2YUXZmX+fuP18wny9xmIUhCR84B2Vd0mIuf0062vVcF7sqCqag0QXyfGBpsYqtCSaUFuylxaWkpdWRkVra3dbQ1lZSwqKRlQllTkLS0tpayujtaKiu62soYGShYtyspnFYS86Y6fCl4/z2g0yg9efpnWFSsA2FlRwQ+qq/lUczOjR48OfP5+4/XzyfTvMwwJ8aYD54tIC/AL4FwR+VmvPq3AeAARGQYcCXQEND8j5BQVFVGyZAnV48axccgQqseNo3TJkoxtthUVFbGkpIRx1dUM2biRcdXVLMni5mUQ8obp82xqaurzTri5uTkr8/cbr59PkL/PSCwWbEkCd6VwdR/eR5cDH1bVy0RkEXCBqsoAw8UsdbZ3clnmaDRKc3Mzxx9/fMonhBd50xnfT8Imr1dvmZaWFjZv3syMGTOYMGFCv/06OzuZU1fXvVIAGFddzYZFi5IeJxqN0t7eTklJSSi+L6+k8/lnQl53pRC+egoicj3we1V9AKgFfioif8JZISzK5tyMcDJ69Gg++tGP5uz4XgmTvDX19dy1axe7zzqL0tWrWTpmDFXz5qXW/6GHkvaP3wnXVlfTdtZZlDU0pHQnPHr0aCZPnpyzNzlev98g5A18pZBhbKWQBoUms8k7eKLRKNPvvpu9N93U3Xbk1Vfz5CWX9Gnzj0ajfKK2lo5bbuluO+qqq3h8yZI++ye+z8+VUT6QwSI7fa4ULKLZMIwB2bZtG3tnzuzRtnfWLJ555pl++0dnz+7RFp09u9/+ceJ3zrloCsoXTCkYhjEg+/fvh8cf79n4m9847X0QiUT4wJo1Pdo+8MtfEolkNvQoGo3S0NAwYFyDkTqmFAzDGJCRI0cy8rnn4PrrYdMmuP56Rj7/PCNHjuyz/5QpUzj1iSeYsHgxQx5+mAmLF3NKQwOnn356xuaU6xHNYcWUgmEYAzJlyhQ+3tTEuJdeIvLss4x76SWmNTX1e5EvKiriMxddxIING7jp059mwYYNzLvoooyZhXI9ojldglgZZdX7yDCM3CB+kf/IXXcxZvVqdo0Zw8SlS5Ne5OdVVRFdsIDm5mYqM+zi29TU1GdEc3Nzc6i8xzJJfU0NLXfdRcmuXewZM4Zjli5lXlVVxo9jKwXDMFJiXlUVF2/cyLHr1nHJpk0pX5D88HAsLy/nyV5RuQ1lZRx//PEZP1YYiEajbFm5klFtbXwkFmNUWxtPrVzpy8rIlIJhGCnjxTvIzyypuR7R7JVt27Yxfu9elgOzgeXA+L17B/TmSgczHxmGkXESs3oCtFZUUFtdzYJoNGmcghfi5qn29nYW5WhEc6pEIhE+0avtk8CrPhzLVgqGYWQcr7mM0mX06NFMnz7dN4WQaipvv5kyZQqPHnlkj7ZHjjySKVOmZPxYphQMw8g45eXllD35ZI+2soaGnLL5h6noUlFREROvvJJry8rYMGQI15aVceyVV/qiDM18ZBiDIGzlO8NCurmMwkIQ5i+vJJrLLvLRXGZKwTDSpKa+nto9e2ibPp2yujqWlJQkTRBXaFTNm8eCeC6jAbKdho1k5q9surwGkRDPzEeGkQZhK98ZVvzOZeRXMFc+mL/SxZSCYaRBUBupRv/4meYibEWXgsSUgmGkQaHeSYbFGyeINBdV8+axfuFC1r75JhsWLSoY02BQNZoPBx4HDnOPuUZVr+vV54PAT4AiYCjwn6r6UBDzMwyvBLWRGqaN7PqaGvbU1jK9rY26sjJKlizxJc1CKgSV5iJsRZeCIKiVwj+Ac1X1FOBUoFJEpvbqcy2gqnoaTtW1OwOam2Gkhd93knHzyIj587OeBTRsCegKLc1FkASiFFQ1pqpvuE8Pcf96J0SJAe9zHx8JDK6kmmEEgF8bqWG7CCe7M88GhZbmIkgCK8cpIkOBbcAk4A5V/Uav18cAG4HRwAhglqpu62OcKqAKQFWnvPPOO4Oa17Bhwzhw4MCgxsg1Ck1mP+Xt6Ohg+/btnHTSSSn5r6fav6GhgQOzZ1PR1dXdtnHIEA555BGm99rg7o0f8kajUX70sY9RvXNnd9uKD36Qy37726z57YPzecbNa9mcR5Bk4vs99NBDoZ9ynIHXaBaRIuA+4Cuq+kJC+1VARFVvFpFpQC1wsqp29TMUWI3mtCg0mf2St6a+nrtaWthVUsKYPXtYeswxSU1IiTb5JwewyXd2dlI3Zw4rWlu726rHjWPRhg0D3g37JW99TQ27a2s5q62NhrIySrO4p5CI/Z69E6oazaraCWwGKnu9tARQt88W4HCgONDJGUaKRKNRVm7ZQtuoUcQ+8hHaRo1i5VNP9Wve8WoOCqN5ZF5VFQvXr+fNtWtZtGFDKBSCkXkCUQoi8gF3hYCIDAdmAY29uv0VmOn2OQFHKfiRBNAwBs22bdvYO348LF8Os2fD8uXsHT++31TG6djkg7gIe3Ux9TsYzUhOEJXXglopjAEeE5HngN8Bm1T1QRG5XkTOd/t8DVgqIs8Cq4GLVTVY25ZhpEgkEoFP9Epm/MlP9ts/XW8ZPy/CYUr4ZgxMUDWpA99TyDC2p5AGhSazF3lTjQvo7Ozk46tWsfemm7rbjrz6ap5avLjf9wVlk09F3mg0SqVqd8I3gHHV1axfuDDnNmwL4fccjUbRysr37DEtXL8+re8r2Z6CJcQzDBcvCe6Kioq4cuJE7rr2WnaffTalTzzB0mOPTblm8aIM1yz2SlgTvhl9E2RNaktzYRikl+Cuat48Nl54IWvfeotNF12UUvBaWGzy+ZKmIwgbexgIMljPlIJhkH6Cu7Bc5L2SDwnfgrKxh4EgvdFsT6EA7JG9KTSZU5G3s7OTOXV177Gxb8ixOgDgfQ+lubmZ47NszvJKpm3suUI0GqW9vZ2SQRbZCVWcgmGEkXy4c06HsK10UnWRDVvajaDwuyY12EazYXSTy5XC8gEvWVjLy8upKyujImGl0FBWxqIc2xMJI7ZSMIwEwnbnXCjkQ8R3vmArBcMwfCPVuI+4OagDeBE4mYFdLhML2S/ysZB9oWFKwTAMX/BqDvrqccfx0tln88r8+Yxfs4bjn3iClSlEfPtdyL7QMPORYRgZx6s5KBaL8fvzzmNnbS1dc+eys7aWbeedR457R+YkphQKjEIJ9jGyi1fvoKamJjoreyZO7qysHNCbqKWlhTvvvJOdCXUejMFhSqGAKKRgHyO7eI3ATSfC+oZLLmHNJz7BCV/9Kr88+2xuuOSSwU/cMKVQKIStvKMRDrymzk6VuHfQNWVl3ByJcI2bALC/zWCvcSItLS0Mf/RRbjl4kDnALQcPMvzRR2lpacmoHIWIbTQXCEEm1DJyAy8bwenQfvTR3Lt4MbvPOovShgaWHn100v5e4kQee+wx5hw82KNtzsGDbN68mYsvvjgT0y9YbKVQIASZUMsIP36vHOMJBttuuIGuOXNou+GGARMMQupxIueeey4bhg7t0bZh6FBmzJgx6LkXOqYUCgQL9jES8TtNRLoJBlNlwoQJvDVzJlcNHcoG4KqhQ3lr5kwmTJiQkfELmUDMRyJyOPA4cJh7zDWqel0f/QT4FhADnlXVi4KYX6FgwT5GHL/TRJSXl1NWV0drRUV3W1lDA8cvWpSR8QGWr1pFS0sLv//971lw5pkFoRCi0SiNjY2U+piXK6iVwj+Ac1X1FOBUoFJEpiZ2EJHJwH8B01X1JODKgOZWUASRUMvon5aWFlatWuWbC2WqLsd+rxyDSjB4zDHHcNlllxWEQsjbcpwicgTQAPy7qm5NaP8e0KSqP/YwnKXOToMwyZxqGoTBEBZ5r/jGN3hk+HD2ffazjLrvPma99RY/uPHGjI2fuHH8ZIobxy0tLWzevJkZM2b4cmENIjV3WL5fP8nLcpwiMhTYBkwC7khUCC7Hu/2eBIYC31LV9X2MUwVUAagqxcXFg5rXsGHDBj1GrhEWmVffdhs777iDqa+8wprx45lw+eVcuGxZ0vd0dHSwfft2TjrppJRPhjDI++c//5lHjjiCvT/6EQB7587lkaVL2bt3L8cdd9ygx+/o6OC1u+/uvmhUtLayYtUqhlx2GUcddVSf70n8/Del+Pl7pbi4mMmTJ2d0zN6E4fv1m8bGxj73gNrb2zP++WZjpVAE3Ad8RVVfSGh/EPgnIMA44AngZFVNtg62lUIahEHmdO580rkThnDIu2rVKqpPP52uuXO724Y8/DA3/OEPGXGh3Lp1KyPmz6eiq6u7beOQIby5dm2fLsf5VKQmDN+v33R2dlI3Z857vq9FGzaktQILVZEd9yK/Gajs9VIrcL+q/lNV/wK8BPh7i2FkDa/eL7kefHfuuecy6r77erSNWrcuYy6UcZfjDpy7qSjJXY7zpUhNoaRtCdJ7MBClICIfcFcIiMhwYBbQ2KtbPTDD7VOMY056OYj5GcHjNW4i1y9iEyZMYNZbb3Hk0qUMefhhjly6lFlvv50xO35RUREvnnMOxy5dyjkPPcTEpUvZfs45/V408iFupaa+nkpVZh84wJy6Omrq67M9JV+ZV1XFwvXrOeSRR1i0YUNGAw0TCWqlMAZ4TESeA34HbFLVB0XkehE53+2zAfi7iGwHHgOuUdW/BzQ/I2C83vnkw0XsBzfeyEOXXMINf/gDDy9enNFN5mg0ytZjj2VvTQ1dc+eyt6aGrRMn5m2RmnhwXOuKFXRVVNC6YkVKwXG5ThDeg4HvKWQY21NIgzDJ7MU7pb6mht21tZzV1kaDm0snV/YU/Gbr1q3MHzGCroS4gCEbN7L2zTeTpjEJwjvID9KVNx/IxO85VHsKhpGIl/KX8eXzm2vX+rp8zkXSyTIKuVt+NF15jYGxhHhGThG/iBk9iQeL1VZX03bWWZQ1NPgSLBYW4vLede217D77bEqfeIIlZWV5K2+QmFIwjAS8BtP5HXznZfx4ltH29nZKBsgyGsR8/Obo9nY+v2oVpd/5DrvHjOHopUuzOp98IalSEJFhwPnAp4FTgCKgE3gWeBioV9UDfk/SMILAayrp+poadt11F2ft3s3q0lLGLF2aUZNWOuP7WbPYb3m9EHdR/l58T7GtjeraWqILFuRcnEXY6HdPQUT+Dccl9N+APwPfBi5z//8ZWAq8LCKXBTBPw/AVr3EQ0WiUv6xcyVfb2jiiq4ur2tp4eeXKjKae/svKldzQ1sacri5uyPD4Qc3Hr1xPue6iHGaSrRSOBz6qqrv7eO0+4DsiMgb4mi8zM4wA8VqEaNu2bew99FBOX7yYV+bPZ/yaNVT87//yzDPPMHPmzEHPZ9u2bczcu7dH26y9ezM2fhDzScz1dNOqVRnN9eR3ltdCpt+Vgqp+rR+FkMgeVb06w3MyjMDxGgexf/9+6v7lX9hZW0vX3LnsrK2l7rzz2L9/f9LjpFr+MhKJ8Hivtt+47QON70eEr9f5tLS08Mjw4T3iJh45/PCMlcvM9TiLMJPWRrOIfBj4EnARUDZAd8MIPd0XmV5xEP1dZEaOHMnr8+f3aHt9wQJGJLHl19TXO9XIpk+nrK6OJSUlVM2b12ffKVOmsPHII7l+716mAVuAV448ki+cfnq/49fX1NBy112U7NrFnjFjOCaDNn+v83nsscfY99nP9mjbd8EFGS2XafVB/CHlOAU3VcUyEXkG+CNwBpDZlIqGkUW8xEFMmTKFozZt6tF21KZNTJkypc/+XiNwi4qKmHbllbSXlnI/8GppKR+/8sp+L3zRaJT7770Xrazk67/6FVpZSf2992ZsDyI+n31lZTwfibCvrCzpfNLN9ZTqSiqO1QfJPAN5Hx2C4310MTAH+BOwGpgAiKq2+z1BI7cIk4tmOqQaB1FUVMRXJk3q4Se/dNKkfueUrDxlf8drP/po7rv00pQK32/bto0/nn027bW1AOycO5e3Fi8ecA/Cy+cZvzNvbm6mcoAI6Hiup0eWLmXfBRcwat26AXM9eVlJGf4xkPloD9AF3A1cp6rPAIjIf/g8LyMHSeek9nJR8uoy6jfxuIDm5maOv+iipPP3Wp4ysfA9QNucOdRWV7MgGu3T5TISifBqL3PWqwsWQIbMWXG8BA/+4MYbee6551i3bh2fmz+fD3/4w/32TVxJAbRWVCSV1/CPgcxHz+HEJnwMOFNE7Nsx+iSdBGXx8oIj5s8fsLxgWFNnp5omwmt5Sq+F76dMmcLoXuas0Rk0Z6VDTX09S7dsoXb+fC596qmkWUy9ymv4R1KloKrnAMcBG4Grgd0i8r/ACOAQ32dn5AxeT2qvF/m4y2hivYBc80uvmjeP9QsXsvbNN9mwaFHSu3KvuX3i5qyya69lyIYNlF17LV9J05yVCbwqHctllBpB1I8YcKNZVXeq6g2qOhmYCezCMSk969ZVNgzPJ7XX4KPy8nJ+OmoUdwBvA7cD94walXMXDb9WFuAonY0XXsgjhx7KposuyqjS8YpXpZOOvOngdSM7TARVP8KTS6qqNgANInIF8Fngi77Mysg5vCZk8xp8FIvFKAWWu89n4yxdczz1e1J67FmkmMso1TQXfifQ87qHAunJ64Vc3sgOcs9lIO+jCE46i5OBZ1T1bgBVfRvHC2l1KgcRkcOBx4HD3GOuUdXr+uk7H/glcKaq/j41MYww4OWk9hoX0NTUxOx9+3q0Vezbl9R7J1/wS/FVzZtHRUsLmzdvZsbcuRmrAgfpKx2/suDm+kZ2Ot5r6TKQ+ej7wAqgFPhvEVmR5nH+AZyrqqcApwKVIjK1dycRGQVcAWxN8zhGlvGrPkJ5eTmbRo3q0bYxB81HXvCyEZ/u+BsWLuSM5ctZL5Lx8b3sofhNrm9kB7nnMpBSEOCTqio4+wkXpXMQVY2p6hvu00Pcv75uf24AvodjNjZykHSCj1JRIrFYjN3A9cAm9/9u8td85Le3VVDeXGEp4pPrG9lB7bnAwHsKR6pqE4CqbheRo9I9kIgMBbYBk4A7VHVrr9dPA8a7tZv7zackIlVAlTsniouL050SAMOGDRv0GNmko6OD7du3c9JJJ6W0DO7o6GDLli186EMfyviy+bbVq7lj505emTqV8WvWcPmECSy78MKMjN3Y2MgX9u3jDGA78BXgd/v20d7ezuTJk5O+Nxe/48bGxj434jMl72DGDxupyFtcXMwVxx7L7StW0DptGuO2bOHLxx3HpEmTAprl4PnmpZdyWUeHE9dz+eW+mb2S1mgWkX3AR3i3luczwGkJz1HVl70cUESKcLKsfkVVX3DbhgC/Bi5W1RYR2QxcncKeQkHXaE4M5noyxfz/Xvp7IRqNUqnabbMFGFddzfqFCzPy4+3s7KRuzhxWJGxMV48bx6INGwa8W8rF79hveePjL2tt5UWcTcNbUxw/bHj5fnO1JnUi2a7RPAIntUX87304tRTizz0b5FS1E9gMVCY0j8L5XW4WkRZgKvCAiJzhdfxCIZ38/3tqa1nW2srwri6uzLC5wG+bbaFlxYzLe01ZGTdHIlwzwEZ8OuO/eM45HLt0Kec89BATly5l+znn5O3nGScs5qwwk9R8pKopJ8xLhoh8APinqnaKyHBgFtCdWF1V9wLFCf03k9pKoWDxmv+/qamJrtZW7sDRuLcDsdbWjHkvpOOC6JXE3DuLUrzTi0ajNDY2Upqj9YoPB06OREjVqz5VeaPRKFuPPZa97spu79y5bK2uJpoj3jhhJUzlStMlIxf9FBgDPCYizwG/Aza5ewfXi8j5Ac0hr/Ca/7+kpIT9Q4eyHMfHfzmwf+hQjk6SZM0LQW2EebnTi3vvHJg92xfvHT+Jr+wSK50NtLLzIm+ue+OEkXhw2fwRI3wNLvObgfYUzgTmqur17vMdOLEG3V2yfDdf8HsKu3v5+fe3R7B161aGf+5zVCZ83+sjEd5ety6jfs5hsdlGo1G0svI9NvmF69fnxJ3w1q1bGTF/PhVdXd1tG4cM4c21a/v8vrzK29nZyZy6uvfsAW3wIWjMC+ncaYfhHPZ7Ty2RbO8pXAO8kDgWTjDbUqAG+MagZmYMCq9+/lvGju3R9tTYsRl3yQuLzTbXa/h6jcvwKm8Y00r4HZfhJ/m08hpIKXwUeDjheZeqPqqqjwK34hTaMbKIl1w6hbRR69W8FjbicRn/CdwC/BfJ4zLSkdfv4LJ8yIKbKrkeB5HIQErh/fQMJDsn4fE/SdgcNsJPfGVxyCOPDLiyyHVyXQk2NTVxxGGHce/ixVzz0EP8fPFiRhx2WNI7//YTT+SqoUPZAFw1dCivnnhiymkl/FghFFIW3CCDy/xmoOC114APATsAVPXZhNdOAP7u07wMn0g1YVo+kMs1fEtKSqj7zGfY695dvzJ3LnVLl/KlfhwDotEoR2/fzrKDB9kOLD94kFu3b8+aN5FX77jy8nL+e9Qotu7d2+0d99KoUXwzh+60/U7oFxQDrRTuA1a6Ce26cd1KbwbW+TUxw8gEXmv4hiW18p49e3j9ggt6tL3+uc/R3t53Bdz4Rfgo4CxgNP7caaf6+Xg1ZyVmwY17x5WSe2lMwrKnNhgGUgrVwFHAn0Xkf0TkOyKyCidwrRjoM9OpYeQiYXIpLC8vZ/TGjT3aRm/c2O9FNYg9FC97BF7Nd8my4BrBMlDltTeA6TgX/+HAme7/bwHTVXVf/+82jNwhiPKUXojFYpzx4INMWLyYIQ8/zITFi5ny4IP93jn7vYeSzkawV++4XHYMyCeSxinkAAUdp5AuhSZzKvJu3bqV+SNG0JUQkT1k40bWvvlmVuo1xOMUzujqYjtwEvC7JHEKcaLRKO3t7ZRkeA/Fa9xEOtTX1PCXu+6idNcudo8Zw8SlSwd0hohGo+zZsydnI9bTIWtxCiJyhYgc1t/rbp/D3CpshpHThM2lMH7nnLhHkMqds9c9FK/zSSTTd/LtRx/NvYsX8/WHH+bexYtpHyDaPpcj1sNMMu+jUuBPIvIQ8BvgJWAfTvK643HcU+cC9/g8R8PwHb/LU6YzHy+V6XJ9PnHzXdsNNwDQNmdO0spoieYsgIrWVqpra4kuWJATEethZqA0F8XAxTgX/w8DRTguxM8BDwH3qGo23VLNfJQGhQ4tlDUAAB2iSURBVCZzLqdWTmc+fn6/fn0+Xs13QZizworf5qOBsqS+hlOS8/uDmoFh5Ah+1QhOl0KZj9csu+Xl5dSVlVGRkOupoayMRbYxPWiCypJqGIbRL14jgnM9Yj3MmPdRgZlSoPBkNnn7J2z5/72ap/zytgqKbGWFTdt8ZBhG/pJYnrUuw+VZ08WreSqX07bU1Nc7m+vTp1NWV8eSkpKMJyVMBzMfGUYBkutZSXOdsAVLJpLSSsEtp/mWqr4hIkOBLwIHgZ+palfyd4ObO+lxnAI9w4A1qnpdrz5XAZcCB4BXgcWqutOLMIaRb/hl3vGasM7ILMnqL2T78091pfAgMNl9/G3gauAqnKR4qfAP4FxVPQU4FagUkam9+vwBOENVPwKsAb6X4tiGkTX8TKDnZy4mSyuRXcIWLJlIqnsKxwN/dB//K/Bx4A3gReCrA71ZVWNuf4BD3L9Yrz6PJTx92j2OYYQWP23yieYFgNaKiqTBXF6Je+9ck5hWwrx3AiNswZKJpKoUDgKHisjxwF5V/auIDAFGpnog1+y0DZgE3KGqW5N0X0LPim+J41QBVQCqSnHx4Or8DBs2bNBj5BqFJrMf8nZ0dPDa3Xf3iKhdsWoVQy67jKOOOmrQ4zc2NvZpXmhvb2fy5Mn9vMshVXn3T5zIL5YupW3aNMq2bOGKiRNz8neRq7/nb156KZd1dLBjxw5OvPzylJW93/KmqhQeBhSnEtsv3LYTgb+leiBVPQicKiJFwH0icrKqvtC7n4j8K06Zz0/2M04NTn1ogNhgvQ4KzV0R/I94DZOLI/gj79atW5n6yis92qa1tvL0009nxCZcWlraZzBXyaJFA8qSirzRaJQfvPwyrcuWwYsv0rpsGT+49VY+1dyc9OJUKN9vkJSXl3Pw4MGUZcigS2qfpLqncCnwK6AW+O/43HBSaHtCVTuBzUBl79dEZBbw/wHnq+o/vI5tZJdcLrzuFb9t8n6Xd2xqaqK1qwvuuAPefhtuv53WWCxp/YJC+n4LmZRWCu4FusY1GZUAu1R1c6oHcb2X/qmqnW7VtlnAjb36nAb8CKhU1b7LSxmDJhqN0tjYmPFUw3EXx2WtrbwIXNnayq15nKAsiIR16ZR3TPX7LSkpYej+/Rx0E9AxezZDr7qKo5OU+7QEdIVBSisFESkSkXuBt3GqriEi54vI/0nxOGOAx0TkOeB3wCZVfVBErheR890+N+HsUfxSRP4oIg94ksQYkLg3y+wDBzLuzdLU1ERXayt34PxIbgdira15XTnLSxGZdPFS3tHL97tnz54eyecAuioqBiz3mUgq5T69emeFpRxqIZPqnsIPcbKjTgC2u21bcFxSrx3ozar6HHBaH+3VCY9npTgXIw389mYpKSlh/9Ch3HDwIODU2b1q6NB+7zzTxatN26+VUSJhSBXj9fstLy9nbF0drZXvWnHHPvVURhPQeY3YDWOEdSGS6p7CTOAKVd2F60qqqq8CmT3jDd9IFiyTCfbs2dMjjTFARVdXv3ee6eDVpu3nyiid+fhJU1MTf5s2rUfb3z7+8X6/X78T0HmN2LUI6/CQqlLYi7Ox3I2IfBDYlfEZGb7gd7BMeXk5W8aO7dH21NixGRvf60Uj3TQCqZovwnYRKykp4X3r1vVoe9/atUlXalXz5rF+4ULWvvkmGxYtGjDvjhdzmdebkHTNU0bmSVUp/BhYKyIzgCEiMg34CY5ZycgB/PZm8TuVsdeLRjorIy93/mG7iO3Zs4eF99/PhMWLGfLww0xYvJiFDzww4ErNy56Fl/5eb0Iswjo8pLqncCPO/uEdONHI/4PjKXSbT/MyfCDuzdLe3k5Jit4sXphXVUV0wQKam5tZlOHKXF5t2l6Ltnj1rglbkZfy8nL+eOihPLNqFdtXreIk4NZx47J2UfUasVtUVET7iSdy1a5dzDl4kA1Dh/LWiSeGJhaikEjVJTUGrHT/jBzG71TDflXm8uoC6vWi5DVBXFhrKK9053NrlucD3lxqo9EoR2/fzrKDB9kOLD94kFu3byeaIUcII3X6LbIjIuemMoCq/jqjM/KGFdlJg1yW2a8iLJ2dndTNmdO9UgCoHjeORRs2DHgxC1tN51wsOjOYmsu5/HtOh2wW2ant9XwsjufR33HSXUSAVuDYQc3OCJQgXDT9xK8iLOne+YexhnIuFp1J1xyX67/nMNKvUlDVifHHIvJNHEWwXFXfFJEjgOtxFISRI4TRDzxMuXT83BOJEyZ5wzSfdJRyGH/P+UBKNZpF5FWgTFX/mdB2CNCmqh/wcX4DYeajFIlGo2hl5XvMIwvXr8+azTbxpH7Sx5M6LN9xOvL6WcM3qM/fC6ma48L4ew4Kv81Hqbqk7gd6r5HPBN5Mf1pGkMQ3UjuAJ3DC07PpQhk2P3+/SUdeP4PjEnNVDe/q4sqQfP6puryGzSU4KKLRKA0NDb6mAUlVKSwH1ovIvSJyo5sHaT0ppLgwwkF5eTk/HTWqR26ie0aNyprLYqGd1F7l9Vtp5nquqkKMa/A7Qj9OSkpBVX8KfAzYAbwPaASmuu1GDhCLxSjF0e6z3f+lZC9vT6Gd1F7l9VtpxnNVJf4e9vuQq8ov/A6WDBvpRuinQ6rBa6jqdt5NhmfkGE1NTczet69HW8W+fVkrFB42P3+/8Sqv38FxyXJVHXPMMRk5ht/EHQPa29tZlGMuuF5JFqGf6fM3ZaXgprj+JE4OpO4NClX9YkZnZPhC2CJwIRhvnzDhRd64Ern2rrs4e/dunigtpSyDSrO8vJy6sWOpTPg9PDV2bFZ/D+mQqy64XvEaoT8YUq2ncB1OWoshwAIcV9Q5gCU9zxHCutz2mnsn1/Eq7zvAi7EY72R4HmH9PRh943fuskRSdUndCXxaVV8QkU5VLRKRjwLXqur5Kbz/cOBx4DCc1ckaVb2uV5/DgHuAKThKZ6GqtgwwdOhdUsPiB544n1yMeB0MYXFJ9cJgXC69yBu2iOx0CNP36/f5nqnzNxMuqUWq+oL7+B0ROURVf4tjTkqFfwDnquopwKlApYhM7dVnCRBV1UnArfQq15mLxL0F5o8Y4au3gBdGjx7N9OnTc/YCUCgE5Z1VaCs1PwnifA/i/E1VKfxZRE5yH78A/LuIfAHH3X1AVDWmqm+4Tw9x/3ovUT6Dk44bYA0wU0T61GS5QJDeAkb+UWjeWblOPp3vqW40X4uT5gLgP4F7ceop/0eqBxKRocA2YBJwh6pu7dVlLPAKgKoeEJG97jFf6zVOFVDl9qO4uJjBMGzYsEGP0ReNjY19egu0t7czefLkjB/PC37JHFa8yNvR0cH27ds56aSTshoZW1xczLFXXMGK229nWmsrW8aN47gvf5lJkyYlfV9HRwdbtmzhQx/6UN5H9sYJw+85yPPdb3kHVAoiMgQnvuVpANdslPyX2QeqehA4VUSKgPtE5OQEkxT0bd96z4aHqtYA8dDO2GBtiX7ZI0tLS/v0FihZtCjr9s8w2WCDIJ20D3eGIO3D7M9/nuinPkVzczMLXJt/Mjl61ES+884BayLnC37+nlPdIwjyfM9gmos+GdB8pKpdwP2qmhEHCFXtBDYDlb1eagXGA4jIMOBIoCMTx8wGQXoLGIMnrGk3UrX555P5Iix42SPIp/M91T2Fx/vYGE4ZEfmAu0JARIYDs3CiohN5APiS+3g+8Gu3uE/O4rUGrpE9cj3tRjrlR43+SUfJ5sv5nuqewk7gYRG5H8fu332xVtXqFN4/BviJu68wxHmbPigi1wO/V9UHcOo3/FRE/oSzQsh8VEYWCFu+faNvggru88tlMcjgpkIg3QjifDjfU1UKw4H42mmc14Oo6nPAaX20Vyc8fhsnMM4wAieItBt+5v/3Wn40XcIYd+NHkZ1CVrIpBa+FmNAHr4WRQpM5DMFc6Qajeb0I+xmc2GMj+8kns76R7Xc9iJr6emp37+6hZMNgEvK7nsKASsENVPun+/gseu5DPKWqBwY1u8FhSiENCk3mMMibTg3idC96fsgbjUapVKV1xYrutnHV1axfuDArrq9BFdkJY8R3VovsiMi/A/+T0LQR+Ln7dx/vbgwbhpEEr8FoYfOGCttGtkV8+8dA3kdfBL6f8PwfqjpeVccDM4FLfZuZYeQRXhPQhc0bqry8nLInn+zRVtbQkLUIa4v49o+BNponquqzCc8T6yk8Cxyb+SkZRn7iJXV22FKdB7WR7WU+hVSPI45fG+uJJN1TEJE3gBJV3d/HayOAPao60peZpYbtKaRBocmcq/LW19Swu9dFL1t7CnHCZmMvpKy/mdxYT7anMNBK4QWgAmf/oDeVwItpzcgwjAGZV1VFS0UFmzdvZu6MGUyYMCHbUwqlH36Oe1CmROIeE0BFayvVtbVEFyzI+Eb/QEphJXCniMSAB1S1y82F9BmcWt9XZXQ2RsERNr/3MJF4Z7g+BLmYwoafcR9hI9keU6aVdNKNZlX9Bc5G88+At0WkDSc53j3ALaq6OqOzMQqK+poatLKSEfPnUzdnDvU1NQO/qUAIm/dR2Ci0zyfIjfVUEuLdDJQB/wJcA5wPjFPVmzI+G6NgKLST2ith8z4KG4X2+QRZPjWlNBeq+jqwIeNHNwqWIJfDuUiu52Lym7B5ZwVB3Hutvb2dRT5urKeaJTUviUajNDQ00NnZme2pFBzmZ56cIO4Mc9l8F+Sdc5gIohxnweY+ClselyAJi4tmui6XXgmLvOmQjgtoKvIGlSbCbwrJJTWO32kuUs2Smlck5koHaK2ooLa6mgXRaE6dELmOl2CuQsUvF9B8Md+NHj2ayZMn56zSDyMFaT4KWx6XQqYQc8uEATPfGf1RkEohbHlcDCNoCtUmbwxMIOYjERmPE9tQCnQBNap6W68+R+LEQ3zQndf3VXWVH/MJWx4Xw8gGZr4z+iKolcIB4GuqegIwFbhcRE7s1edyYLuqngKcA9wsIof6NaF4PdVHDjkkp+upGsZg8Gq+i0ajbN261Tz28phAlIKq7lLVZ9zH+4AdwNhe3WLAKBGJACNx6jT7WsAnCPcuw8gXctmFNV8Iwo0+cO8jETkGp17z1l4v3Q48ALQBo4CFqtrVqw8iUgVUAagqxcXFg5rPsGHDBj1GrlFoMpu8g6ejo4PX7r67R0K2FatWMeSyyzjqqKMyeiyvFMr3u/q229h5xx1MfeUV1owfz4TLL+fCZcsyfpxA4xREZCTwG+Dbqrqu12vzgek4SfaOAzYBp7jR1P1hqbPToNBkNnkHTzrlRIOiEL7fTMeVpF2OM5OIyCHAWuDnvRWCyyXAOlWNqeqfgL8AHwpqfkZmMJtzfmIurNklyFxPgSgFd5+gFtihqrf00+2vOCU+EZESoBx4OYj5GZnBbM75i7mwZpcglXJQewrTgS8Az4vIH922b+K4n6KqPwRuAO4WkedxljXfUNX8XhPmEUEWATGyg7mwZo8gy48WbO6jOIVgj+yN2Zz7J1ezhiZSaL/pQpI3U7meQrGnYOQ3+WBzNvOXEXaCcKM3pWBkhFy3OVvRH8NwKMgsqYY/hNHmHI1GaWxspHSANCb5kjXUMAaLrRSMjBKmrKdxc9CB2bMHNAela/4yF1wj3zClYCQlVy96Xs1B6Zi/bA/CyEfMfGT0S31NDXtqa5ne1kZdWRklPlVG84N0zEFezF/mgmvkK7ZSMPok1zde0zUHpWr+CjLC1DCCxJSC0Sdhveilas7y2xsqH1xwDaMvzHxk9El5eTl1ZWVUJCTgaigrY1EWL3o19fXU7tlD2/TplNXVsaSkJGkdjLg5qL29nUUZLuweZISpYQSJKQWjT8J20YtGo9Tu2UPrihUAtFZUUFtdzYJoNKkN38/C7mF0wTWMwWJKweiXMF30mpqaaJs+vUdb21lnZT2OIL4HYRj5gu0pGEkJS9xBeXk5ZU8+2aOtrKHBbPiGkWFMKRhZxcvG8ZKSEsZVVzNk40bGVVezZIAoZcMwvGPmIyNreI2DqJo3j4qWFjZv3syMuXOZMGFCxueUD1lSDWMw2ErByArpxEHU1NezcMMGlp9xBrJ+PTX19Rmdk0UoG0ZAKwURGQ/cA5QCXUCNqt7WR79zgJXAIcBrqvrJIOZnBI/XiON0vY9SxSKUDcMhqJXCAeBrqnoCMBW4XEROTOwgIkXAncD5qnoSsCCguRlZwGvwVzLvo0wQ1mA9wwiaQJSCqu5S1Wfcx/uAHcDYXt0uAtap6l/dfu1BzM3IDl4jjv32PrIIZcNwCLwcp4gcAzwOnKyqrye0x81GJwGjgNtU9Z4+3l8FVAGo6pR33nlnUPMZNmwYBw4cGNQYuUaYZO7o6GDHjh2ceOKJA5ppblu9mttbWmidNo1xW7bw5WOOYdmFFw54jFTlXX3bbbTcfjvTWlvZMm4cx3z5y1y4bFnKsoSFMH2/QWDyeufQQw+FfspxBqoURGQk8Bvg26q6rtdrtwNnADOB4cAW4NOq2pRkSKvRnAa5LHM0GqW5uZnjPQTTea3R7HX8sJHL3286mLzeSVajOTCXVBE5BFgL/Ly3QnBpxdlc3g/sF5HHgVOAZErBKDD8jiC2CGWj0AnK+ygC1AI7VPWWfrrdD9wuIsOAQ4GPAbcGMT/DMAzDIaiVwnTgC8DzIvJHt+2bwAcBVPWHqrpDRNYDz+G4rf5YVV8IaH6GYRgGASkFVW2gH/tVr343ATf5PyPDMAyjLyyi2TAMw+jGlIJhBEiqCQANI1uYUjCMgLDcSkYuYErBMAIgnQSAhpENTCkYRgBYbiUjVzClYBgBYLmVjFzBlIJhBIDXBICGkS2s8pphBMS8qiqiCxbQ3NzMohzOrWTkN6YUDCNALLeSEXbMfGQYhmF0Y0rBMAzD6MaUgmEYhtGNKQXDMAyjG1MKhmEYRjemFAwjQCwhnhF2TCkYRkBYQjwjFwiqHOd44B6gFKeqWo2q3tZP3zOBp4GFqromiPkZht8kJsQDqGhtpbq2luiCBYwePTrLszOMdwlqpXAA+JqqngBMBS4XkRN7dxKRocCNwIaA5mUYgWAJ8YxcIRCloKq7VPUZ9/E+YAcwto+uXwHWAu1BzMswgsIS4hm5QuBpLkTkGOA0YGuv9rHAZ4FzgTOTvL8KqAJQVYqLiwc1n2HDhg16jFyj0GQOg7zFxcUce8UVrLj9dqa1trJl3DiO+/KXmTRpUsaPFQZ5g8TkzSyRWCzm2+C9EZGRwG+Ab6vqul6v/RK4WVWfFpG7gQdT2FOItfVaknuluLiY1157bVBj5BqFJnOY5I1GozQ3N3O8jwnxwiRvEJi83ilzVq2Rvl4LzPtIRA7BMQ39vLdCcDkD+IWItADzgTtFZF5Q8zOMIIgnxLMMqUZYCcr7KALUAjtU9Za++qjqxIT+d+OsFOqDmJ9hGIbhENSewnTgC8DzIvJHt+2bwAcBVPWHAc3DMAzDSEIgSkFVG+jHftVP/4v9m41hGIbRHxbRbBiGYXRjSsEwDMPoxpSCYRiG0U2gcQo+kNOTNwzDyCLZjVPwichg/0RkWybGyaW/QpPZ5M3vP5M37b8+yXWlYBiGYWQQUwqGYRhGN6YUoBArnRSazCZvfmPyZpBc32g2DMMwMoitFAzDMIxuTCkYhmEY3QReZCdoRORw4HHgMBx516jqdSIyEfgFcBTwDPAFVX1HRA7DqSc9Bfg7Tq3olqxMfhC4pU1/D/xNVc/LZ3nddOv7gIPAAVU9Q0SOAuqAY4AWQFQ16mbsvQ34FPAmcHG8KmCuICJFwI+Bk3FidRYDL5GH8opIOY5ccY4FqnF+s3knL4CIfBW4FOe7fR64BBhDQOdvIawU/gGcq6qnAKcClSIyFacW9K2qOhmIAkvc/kuAqKpOAm51++Uiy3DKnsbJd3lnqOqpqnqG+/w/gUddeR91nwPMBSa7f1XA/w18poPnNmC9qn4IOAXne85LeVX1Jfd7PRXnwvcmcB95Kq9bgfIK4AxVPRkYCiwiwPM375WCqsZU9Q336SHuXwyn7Ge8sttPgHhBn8+4z3Ffn+nefeQMIjIO+DTO3WS8nkXeytsPiXL1lvce93fxNFAkImOyMcF0EJH3AZ/AqU+Cqr6jqp3kqby9mAn8WVV3kt/yDgOGi8gw4AhgFwGev3mvFMAxpbh1HNqBTcCfgU5VPeB2aQXGuo/HAq8AuK/vBd4f7IwHzUrg60CX+/z95Le8MWCjiGxza3gDlKjqLgD3/9Fue7e8LomfRS5wLPAqsEpE/iAiPxaREeSvvIksAla7j/NSXlX9G/B94K84ymAvsI0Az9+CUAqqetBdfo4DPgqc0Ee3uG9uX1o2Z/x2ReQ8oF1VtyU0J5Mpp+V1ma6qp+OYDi4XkU8k6Zvr8g4DTgf+r6qeBuznXdNJX+S6vACIyKHA+cAvB+ia0/KKyGicu/+JQBkwAud33Rvfzt+CUApx3GX2ZmAqzrIyvtE+DmhzH7cC4wHc148EOoKd6aCYDpzvbr7+AmfZuZL8lRdVbXP/t+PYmz8K7ImbDdz/7W73bnldEj+LXKAVaFXVre7zNThKIl/ljTMXeEZV97jP81XeWcBfVPVVVf0nsA74OAGev3mvFETkA663BiIyHOdD3wE8Bsx3u30JuN99/ID7HPf1X6tqztxpqOp/qeo4VT0GZ7n9a1X9PHkqr4iMEJFR8cdABfACPeXqLe8XRSTiOhzsjZshcgFV3Q284nrlgGNn306eypvAhbxrOoL8lfevwFQROcLdG4h/v4Gdv3mvFHBcuR4TkeeA3wGbVPVB4BvAVSLyJxwbXK3bvxZ4v9t+FcmX5rlEvspbAjSIyLPAb4Ffqep64LvAbBFpBma7zwEeAl4G/gTcBfxH8FMeNF8Bfu7+pk8FvkMeyysiR+DItC6hOS/ldVeAa3DcTp/HuUbXEOD5a2kuDMMwjG4KYaVgGIZhpIgpBcMwDKMbUwqGYRhGN6YUDMMwjG5MKRiGYRjd5H2WVMPIBq6P+W+BL6nq9oCO+XXgCFX9VhDHM/ITUwpG3iIibyQ8PQInY+5B9/m/qerP0xz3aeB2Vf1Zkm7zcdKWdysEETkBuB6YARwK7AZ+BXxPVXeJSCWOn/2bOKkKWoH/E5+nmwb+rYTX43xTVX8A3AE0i8htqhpNRzbDMKVg5C2qOjL+2E37camqPhLQ4S8D7kw4/gnA0zipnJepapuIlAJfAKbxbmDWy6o6yV1pfAb4pYg8pap/SRi7XFVbex9QVfeLyKPA54HbfZHKyHtMKRgFi1uI6Js4RUzeB2wALlfVTjdlRi1O2owhOEVsKoFrgTOBH4vID4EfqerXeo17BE5664UJzTcAG1W1O+LUTVlxU19zc1MV1IvIWzjFdP7SV78+2IyzSjGlYKSFKQWjkLkG56J/Fk4SsR/iFCq5BKfy1TCc1MT/BE4D3lHVr4nIdJKbj04AXlfV1xLaZuGsHlJCRIYAnwNG4aR6T5UdOIV3DCMtTCkYhcy/Af8az7IqIiuAF0VkMY4i+ABwnKq+gJM3K1WKcMqD4o47FCd75e6EtqtxVh3DgFWq+hX3pYki0gkMx6m69R99bFS/KCKJewqfUdXfuI/3ucc3jLQwpWAUJK7NfjzwUK8L7BDeTThWCqwRkZE4dXCXq+rB9wz2XqI4d/iAU89DRF7HSc4Yb/s+8H0R+T4wMuG9f3H3FA4HbsFJfd67pORJfe0puIwCOlOYo2H0iSkFoyBR1ZiI/A24oFdBokSqgWoRORZnv+FF4OcMXMRkBzBKRIoTTEiPAhfQM/1zsvm9LSJXAX8SkUo382sqnAA8m2Jfw3gPFrxmFDI/BL4rIvEiJUeLyL+4j2eJyImubf914ADvurPuwSmL2Seq+hbOhm9iBbjlwBwR+a6IlMWPB5S/d4Tucd7GKZB0nQeZPgk87KG/YfTAlIJRyHwPeAT4tYjsA57CqWIGzgbz/Tg2+hdw4gfUfe1WnEIuURH5Xj9j/wjH3RQAVX0Rx/V0MvC8a056HCfv//VJ5lgDnCAisxPaXhKRNxL+boTuIkOzgGTxE4aRFKunYBg+4O5ZbAUuDjCi+RpglKpWB3E8Iz8xpWAYhmF0Y+YjwzAMoxtTCoZhGEY3phQMwzCMbkwpGIZhGN2YUjAMwzC6MaVgGIZhdGNKwTAMw+jm/wEaQYvoHt8a5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_rank1 = data[data[\"rank\"]==1]\n",
    "data_rank2 = data[data[\"rank\"]==2]\n",
    "data_rank3 = data[data[\"rank\"]==3]\n",
    "data_rank4 = data[data[\"rank\"]==4]\n",
    "plot_points(data_rank1)\n",
    "plt.title(\"Rank 1\")\n",
    "plt.show()\n",
    "plot_points(data_rank2)\n",
    "plt.title(\"Rank 2\")\n",
    "plt.show()\n",
    "plot_points(data_rank3)\n",
    "plt.title(\"Rank 3\")\n",
    "plt.show()\n",
    "plot_points(data_rank4)\n",
    "plt.title(\"Rank 4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece ser que por RANK la relación es un poco más clara en algunos casos, se procede a construir una MLP con RANK, GRE y GPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Procesado de datos:\n",
    "\n",
    "- Se remueven NaNs\n",
    "- One-hot encode con rank\n",
    "- Normalizacion de GRE y GPA, de manera que queden en el rango (0,1)\n",
    "- Se parten los datos en input X - labels y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# remove NaNs\n",
    "data = data.fillna(0)\n",
    "\n",
    "# One-hot encoding the rank\n",
    "processed_data = pd.get_dummies(data, columns=['rank'])\n",
    "\n",
    "# Normalizing the gre and the gpa scores to be in the interval (0,1)\n",
    "processed_data[\"gre\"] = processed_data[\"gre\"]/800\n",
    "processed_data[\"gpa\"] = processed_data[\"gpa\"]/4\n",
    "\n",
    "# Splitting the data input into X, and the labels y \n",
    "X = np.array(processed_data)[:,1:]\n",
    "X = X.astype('float32')\n",
    "y = keras.utils.to_categorical(data[\"admit\"],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (400, 6)\n",
      "\n",
      "Shape of y: (400, 2)\n",
      "\n",
      "First 10 rows of X\n",
      "[[0.475  0.9025 0.     0.     1.     0.    ]\n",
      " [0.825  0.9175 0.     0.     1.     0.    ]\n",
      " [1.     1.     1.     0.     0.     0.    ]\n",
      " [0.8    0.7975 0.     0.     0.     1.    ]\n",
      " [0.65   0.7325 0.     0.     0.     1.    ]\n",
      " [0.95   0.75   0.     1.     0.     0.    ]\n",
      " [0.7    0.745  1.     0.     0.     0.    ]\n",
      " [0.5    0.77   0.     1.     0.     0.    ]\n",
      " [0.675  0.8475 0.     0.     1.     0.    ]\n",
      " [0.875  0.98   0.     1.     0.     0.    ]]\n",
      "\n",
      "First 10 rows of y\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Checking that the input and output look correct\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"\\nShape of y:\", y.shape)\n",
    "print(\"\\nFirst 10 rows of X\")\n",
    "print(X[:10])\n",
    "print(\"\\nFirst 10 rows of y\")\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Partición de los datos en training y testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (350, 6)\n",
      "350 train samples\n",
      "50 test samples\n"
     ]
    }
   ],
   "source": [
    "# break training set into training and validation sets\n",
    "(X_train, X_test) = X[50:], X[:50]\n",
    "(y_train, y_test) = y[50:], y[:50]\n",
    "\n",
    "# print shape of training set\n",
    "print('x_train shape:', X_train.shape)\n",
    "\n",
    "# print number of training, validation, and test images\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Definición de la arquitectura de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 00:18:10.918869  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0819 00:18:10.934476  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0819 00:18:10.950102  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0819 00:18:10.981369  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0819 00:18:11.013507  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,282\n",
      "Trainable params: 9,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Building the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=6))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 00:18:17.877936  5384 deprecation.py:323] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0819 00:18:17.927507  5384 deprecation_wrapper.py:119] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22959d8b978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=0)\n",
    "#model.fit(X_train, y_train, epochs=1000, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Score del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 134us/step\n",
      "\n",
      " Training Accuracy: 0.7228571449007307\n",
      "50/50 [==============================] - 0s 0us/step\n",
      "\n",
      " Testing Accuracy: 0.6600000095367432\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Play with parameters!\n",
    "\n",
    "- Activation function: relu and sigmoid\n",
    "- Loss function: categorical_crossentropy, mean_squared_error\n",
    "- Optimizer: rmsprop, adam, ada\n",
    "- epochs? batches?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un pequeño ejercicio agregando dropout..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 00:18:28.673897  5384 deprecation.py:506] From C:\\Users\\rasala\\AppData\\Local\\Continuum\\miniconda3\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,298\n",
      "Trainable params: 11,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=6))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.3)) # now using dropout\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.2)) # now using dropout\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.1)) # now using dropout\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229661eee80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "#model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=0)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 329us/step\n",
      "\n",
      " Training Accuracy: 0.7200000020435878\n",
      "50/50 [==============================] - 0s 80us/step\n",
      "\n",
      " Testing Accuracy: 0.6400000095367432\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejora en el modelo anterior (Tarea)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizará Modelo con 3 capas ocultas (primer modelo utilizado en el archivo) para tratar de lograr una mejor precisión de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Creación de DataFrame que almacenará valores de: Hyperparámetros, Accuracy obtenido para los Hyperparámetros utilizados.**\n",
    "\n",
    "\n",
    "Columnas:\n",
    "\n",
    "epochs = epochs\n",
    "\n",
    "batch = batch_size\n",
    "\n",
    "lay1_siz = layer1_size\n",
    "\n",
    "lay1_act = layer1_activ\n",
    "\n",
    "Drop1 = Droput layer1\n",
    "\n",
    "lay2_siz = layer2_size\n",
    "\n",
    "lay2_act = layer2_activ\n",
    "\n",
    "Drop2 = Droput layer2\n",
    "\n",
    "lay3_siz = layer3_size\n",
    "\n",
    "lay3_act = layer3_activ\n",
    "\n",
    "Drop3 = Droput layer3\n",
    "\n",
    "lout_siz = layer_out_size\n",
    "\n",
    "lout_act = layer_out_activ\n",
    "\n",
    "loss = loss\n",
    "\n",
    "optimizer = optimizer\n",
    "\n",
    "test_accur = test_accur\n",
    "\n",
    "train_accur = train_accur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=('epochs', 'batch', 'lay1_siz', 'lay1_act', 'Drop1', 'lay2_siz', 'lay2_act', 'Drop2', 'lay3_siz', 'lay3_act', 'Drop3', 'lout_siz', 'lout_act', 'loss', 'optimizer', 'test_accur', 'train_accur'), index = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificación de Dataframe creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [epochs, batch, lay1_siz, lay1_act, Drop1, lay2_siz, lay2_act, Drop2, lay3_siz, lay3_act, Drop3, lout_siz, lout_act, loss, optimizer, test_accur, train_accur]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Variación del Parámetro Batch_SIZE\n",
    "\n",
    "Se evaluó el modelo anterior de forma que se variara el Hiperparámetro **Batch_size** para poder detrminar cuál valor es el que presentaba una mejor respuesta con los datos de testing y training. Estos datos se almacenaron en un archivo cvs externo, con el formato del dataset base creado anteriormente, obteniéndose los siguientes valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>50</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>75</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>125</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>150</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500</td>\n",
       "      <td>175</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>225</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>250</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>275</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>300</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>500</td>\n",
       "      <td>325</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.708571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>500</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>500</td>\n",
       "      <td>400</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>500</td>\n",
       "      <td>450</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "0      500     25       128     tanh    0.3        64     tanh    0.2   \n",
       "1      500     50       128     tanh    0.3        64     tanh    0.2   \n",
       "2      500     75       128     tanh    0.3        64     tanh    0.2   \n",
       "3      500    100       128     tanh    0.3        64     tanh    0.2   \n",
       "4      500    125       128     tanh    0.3        64     tanh    0.2   \n",
       "5      500    150       128     tanh    0.3        64     tanh    0.2   \n",
       "6      500    175       128     tanh    0.3        64     tanh    0.2   \n",
       "7      500    200       128     tanh    0.3        64     tanh    0.2   \n",
       "8      500    225       128     tanh    0.3        64     tanh    0.2   \n",
       "9      500    250       128     tanh    0.3        64     tanh    0.2   \n",
       "10     500    275       128     tanh    0.3        64     tanh    0.2   \n",
       "11     500    300       128     tanh    0.3        64     tanh    0.2   \n",
       "12     500    325       128     tanh    0.3        64     tanh    0.2   \n",
       "13     500    350       128     tanh    0.3        64     tanh    0.2   \n",
       "14     500    375       128     tanh    0.3        64     tanh    0.2   \n",
       "15     500    400       128     tanh    0.3        64     tanh    0.2   \n",
       "16     500    450       128     tanh    0.3        64     tanh    0.2   \n",
       "17     500    500       128     tanh    0.3        64     tanh    0.2   \n",
       "\n",
       "    lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "0         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "1         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "2         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "3         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "4         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "5         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "6         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "7         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "8         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "9         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "10        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "11        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "12        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "13        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "14        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "15        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "16        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "17        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "\n",
       "   optimizer  test_accur  train_accur  \n",
       "0    rmsprop        0.60     0.728571  \n",
       "1    rmsprop        0.64     0.720000  \n",
       "2    rmsprop        0.62     0.725714  \n",
       "3    rmsprop        0.64     0.717143  \n",
       "4    rmsprop        0.66     0.722857  \n",
       "5    rmsprop        0.66     0.722857  \n",
       "6    rmsprop        0.68     0.722857  \n",
       "7    rmsprop        0.68     0.714286  \n",
       "8    rmsprop        0.68     0.722857  \n",
       "9    rmsprop        0.66     0.720000  \n",
       "10   rmsprop        0.64     0.714286  \n",
       "11   rmsprop        0.66     0.711429  \n",
       "12   rmsprop        0.62     0.708571  \n",
       "13   rmsprop        0.68     0.717143  \n",
       "14   rmsprop        0.68     0.728571  \n",
       "15   rmsprop        0.68     0.720000  \n",
       "16   rmsprop        0.64     0.720000  \n",
       "17   rmsprop        0.64     0.717143  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del archivo csv con los datos guardados de las corridas\n",
    "df_batch = pd.read_csv('https://raw.githubusercontent.com/rasalav/Tarea4/master/Tarea4_ANN_batch.csv', index_col=0)\n",
    "df_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros empleados para el máximo valor de Testing obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500</td>\n",
       "      <td>175</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500</td>\n",
       "      <td>200</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>225</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>500</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>500</td>\n",
       "      <td>400</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "6      500    175       128     tanh    0.3        64     tanh    0.2   \n",
       "7      500    200       128     tanh    0.3        64     tanh    0.2   \n",
       "8      500    225       128     tanh    0.3        64     tanh    0.2   \n",
       "13     500    350       128     tanh    0.3        64     tanh    0.2   \n",
       "14     500    375       128     tanh    0.3        64     tanh    0.2   \n",
       "15     500    400       128     tanh    0.3        64     tanh    0.2   \n",
       "\n",
       "    lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "6         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "7         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "8         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "13        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "14        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "15        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "\n",
       "   optimizer  test_accur  train_accur  \n",
       "6    rmsprop        0.68     0.722857  \n",
       "7    rmsprop        0.68     0.714286  \n",
       "8    rmsprop        0.68     0.722857  \n",
       "13   rmsprop        0.68     0.717143  \n",
       "14   rmsprop        0.68     0.728571  \n",
       "15   rmsprop        0.68     0.720000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_batch[df_batch['test_accur'] == df_batch['test_accur'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo valor de Training para ese valor de Testing máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7285714282308307"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_batch['train_accur'][df_batch['test_accur'] == df_batch['test_accur'].max()].max()\n",
    "#df_batch[df_batch['train_accur'] == df_batch['train_accur'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De estos datos, se aprecia que el mejor valor obtenido para: **epochs = 500** , es **batch_size = 375**. Para estos valores,\n",
    "\n",
    "**testing_accuracy = 0.68**\n",
    "\n",
    "**trainning_acurracy = 0.728571**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Variación del Parámetro epochs\n",
    "\n",
    "Se procedió a efectuar corridas con el parámetro batch_size = 350, y variando los epochs, para ver el comportamiento de los valores de accuracy que se obtenía. Los resultados encontrados fueron los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.697143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.691429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.705714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.705714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>250</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>300</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>400</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>500</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "0        1    350       128     tanh    0.3        64     tanh    0.2   \n",
       "1        2    350       128     tanh    0.3        64     tanh    0.2   \n",
       "2        3    350       128     tanh    0.3        64     tanh    0.2   \n",
       "3        4    350       128     tanh    0.3        64     tanh    0.2   \n",
       "4        5    350       128     tanh    0.3        64     tanh    0.2   \n",
       "5        6    350       128     tanh    0.3        64     tanh    0.2   \n",
       "6        7    350       128     tanh    0.3        64     tanh    0.2   \n",
       "7        8    350       128     tanh    0.3        64     tanh    0.2   \n",
       "8        9    350       128     tanh    0.3        64     tanh    0.2   \n",
       "9       10    350       128     tanh    0.3        64     tanh    0.2   \n",
       "10      50    350       128     tanh    0.3        64     tanh    0.2   \n",
       "11     100    350       128     tanh    0.3        64     tanh    0.2   \n",
       "12     150    350       128     tanh    0.3        64     tanh    0.2   \n",
       "13     200    350       128     tanh    0.3        64     tanh    0.2   \n",
       "14     250    350       128     tanh    0.3        64     tanh    0.2   \n",
       "15     300    350       128     tanh    0.3        64     tanh    0.2   \n",
       "16     350    350       128     tanh    0.3        64     tanh    0.2   \n",
       "17     400    350       128     tanh    0.3        64     tanh    0.2   \n",
       "18     500    350       128     tanh    0.3        64     tanh    0.2   \n",
       "\n",
       "    lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "0         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "1         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "2         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "3         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "4         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "5         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "6         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "7         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "8         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "9         32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "10        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "11        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "12        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "13        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "14        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "15        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "16        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "17        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "18        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "\n",
       "   optimizer  test_accur  train_accur  \n",
       "0    rmsprop        0.56     0.700000  \n",
       "1    rmsprop        0.56     0.700000  \n",
       "2    rmsprop        0.56     0.700000  \n",
       "3    rmsprop        0.56     0.700000  \n",
       "4    rmsprop        0.56     0.700000  \n",
       "5    rmsprop        0.56     0.700000  \n",
       "6    rmsprop        0.56     0.700000  \n",
       "7    rmsprop        0.58     0.697143  \n",
       "8    rmsprop        0.56     0.700000  \n",
       "9    rmsprop        0.54     0.691429  \n",
       "10   rmsprop        0.62     0.705714  \n",
       "11   rmsprop        0.62     0.705714  \n",
       "12   rmsprop        0.66     0.714286  \n",
       "13   rmsprop        0.66     0.711429  \n",
       "14   rmsprop        0.66     0.714286  \n",
       "15   rmsprop        0.66     0.720000  \n",
       "16   rmsprop        0.64     0.717143  \n",
       "17   rmsprop        0.64     0.717143  \n",
       "18   rmsprop        0.68     0.722857  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del archivo csv con los datos guardados de las corridas\n",
    "df_epoch = pd.read_csv('https://raw.githubusercontent.com/rasalav/Tarea4/master/Tarea4_ANN_epochs.csv', index_col=0)\n",
    "df_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs                              500\n",
       "batch                               350\n",
       "lay1_siz                            128\n",
       "lay1_act                           tanh\n",
       "Drop1                               0.3\n",
       "lay2_siz                             64\n",
       "lay2_act                           tanh\n",
       "Drop2                               0.2\n",
       "lay3_siz                             32\n",
       "lay3_act                           tanh\n",
       "Drop3                               0.1\n",
       "lout_siz                              2\n",
       "lout_act                        sigmoid\n",
       "loss           categorical_crossentropy\n",
       "optimizer                       rmsprop\n",
       "test_accur                         0.68\n",
       "train_accur                    0.722857\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_epoch.loc[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Valor máximo de testing accuracy obtenido**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799999976158142"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_epoch['test_accur'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de los parámetros de la ANN para ese valor son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>500</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.2</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "18     500    350       128     tanh    0.3        64     tanh    0.2   \n",
       "\n",
       "    lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "18        32     tanh    0.1         2  sigmoid  categorical_crossentropy   \n",
       "\n",
       "   optimizer  test_accur  train_accur  \n",
       "18   rmsprop        0.68     0.722857  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_epoch[df_epoch['test_accur'] == df_epoch['test_accur'].max()]\n",
    "# Otra forma de obtener la fila:\n",
    "#df_epoch.loc[df_epoch.index[df_epoch['test_accur'] == df_epoch['test_accur'].max()].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De estos datos, se aprecia que el mejor valor obtenido para: **batch_size = 350*, es **epochs = 500**. Para estos valores,\n",
    "\n",
    "**testing_accuracy = 0.68**\n",
    "\n",
    "**trainning_acurracy = 0.722857**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la arquitectura de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,298\n",
      "Trainable params: 11,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=6))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.3)) # now using dropout\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.2)) # now using dropout\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(.1)) # now using dropout\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2296632f0f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "#model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=0)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=350,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 477us/step\n",
      "\n",
      " Training Accuracy: 0.719999999659402\n",
      "50/50 [==============================] - 0s 0us/step\n",
      "\n",
      " Testing Accuracy: 0.6600000095367432\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Variación de  los Parámetro Dropout = 0 (para las tres capas del modelo)\n",
    "\n",
    "Se procedió a efectuar corridas con los siguientes parámetros:\n",
    "\n",
    "batch_size = 350, epochs = 350, Dropout = 0 (para las tres capas de la ANN).\n",
    "\n",
    "Adicionalmente se procedió a variar los parámetros:layer_activ para las tres capas, loss, optimizer; combinando valores entre ellas. Se utilizaron los valores:\n",
    "\n",
    "**layer_activ = ['relu', 'sigmoid', 'tanh']**\n",
    "\n",
    "**loss = ['categorical_crossentropy', 'mean_squared_error']**\n",
    "\n",
    "**Optimizer = ['rmsprop', 'adam', 'Adagrad', 'Adadelta', 'SGD']**\n",
    "\n",
    "\n",
    "Los resultados encontrados fueron los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.751429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.734286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.405714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.282857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.288571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.702857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.305714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.474286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.697143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.697143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.705714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "0       350    350       128     relu      0        64     relu      0   \n",
       "1       350    350       128     relu      0        64     relu      0   \n",
       "2       350    350       128     relu      0        64     relu      0   \n",
       "3       350    350       128     relu      0        64     relu      0   \n",
       "4       350    350       128     relu      0        64     relu      0   \n",
       "5       350    350       128     relu      0        64     relu      0   \n",
       "6       350    350       128     relu      0        64     relu      0   \n",
       "7       350    350       128     relu      0        64     relu      0   \n",
       "8       350    350       128     relu      0        64     relu      0   \n",
       "9       350    350       128     relu      0        64     relu      0   \n",
       "10      350    350       128     relu      0        64     relu      0   \n",
       "11      350    350       128     relu      0        64     relu      0   \n",
       "12      350    350       128     relu      0        64     relu      0   \n",
       "13      350    350       128     relu      0        64     relu      0   \n",
       "14      350    350       128     relu      0        64     relu      0   \n",
       "15      350    350       128     relu      0        64     relu      0   \n",
       "16      350    350       128     relu      0        64     relu      0   \n",
       "17      350    350       128     relu      0        64     relu      0   \n",
       "18      350    350       128     relu      0        64     relu      0   \n",
       "19      350    350       128     relu      0        64     relu      0   \n",
       "20      350    350       128     relu      0        64     relu      0   \n",
       "21      350    350       128     relu      0        64     relu      0   \n",
       "22      350    350       128     relu      0        64     relu      0   \n",
       "23      350    350       128     relu      0        64     relu      0   \n",
       "24      350    350       128     relu      0        64     relu      0   \n",
       "25      350    350       128     relu      0        64     relu      0   \n",
       "26      350    350       128     relu      0        64     relu      0   \n",
       "27      350    350       128     relu      0        64     relu      0   \n",
       "28      350    350       128     relu      0        64     relu      0   \n",
       "29      350    350       128     relu      0        64     relu      0   \n",
       "..      ...    ...       ...      ...    ...       ...      ...    ...   \n",
       "780     350    350       128     tanh      0        64     tanh      0   \n",
       "781     350    350       128     tanh      0        64     tanh      0   \n",
       "782     350    350       128     tanh      0        64     tanh      0   \n",
       "783     350    350       128     tanh      0        64     tanh      0   \n",
       "784     350    350       128     tanh      0        64     tanh      0   \n",
       "785     350    350       128     tanh      0        64     tanh      0   \n",
       "786     350    350       128     tanh      0        64     tanh      0   \n",
       "787     350    350       128     tanh      0        64     tanh      0   \n",
       "788     350    350       128     tanh      0        64     tanh      0   \n",
       "789     350    350       128     tanh      0        64     tanh      0   \n",
       "790     350    350       128     tanh      0        64     tanh      0   \n",
       "791     350    350       128     tanh      0        64     tanh      0   \n",
       "792     350    350       128     tanh      0        64     tanh      0   \n",
       "793     350    350       128     tanh      0        64     tanh      0   \n",
       "794     350    350       128     tanh      0        64     tanh      0   \n",
       "795     350    350       128     tanh      0        64     tanh      0   \n",
       "796     350    350       128     tanh      0        64     tanh      0   \n",
       "797     350    350       128     tanh      0        64     tanh      0   \n",
       "798     350    350       128     tanh      0        64     tanh      0   \n",
       "799     350    350       128     tanh      0        64     tanh      0   \n",
       "800     350    350       128     tanh      0        64     tanh      0   \n",
       "801     350    350       128     tanh      0        64     tanh      0   \n",
       "802     350    350       128     tanh      0        64     tanh      0   \n",
       "803     350    350       128     tanh      0        64     tanh      0   \n",
       "804     350    350       128     tanh      0        64     tanh      0   \n",
       "805     350    350       128     tanh      0        64     tanh      0   \n",
       "806     350    350       128     tanh      0        64     tanh      0   \n",
       "807     350    350       128     tanh      0        64     tanh      0   \n",
       "808     350    350       128     tanh      0        64     tanh      0   \n",
       "809     350    350       128     tanh      0        64     tanh      0   \n",
       "\n",
       "     lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "0          32     relu      0         2     relu  categorical_crossentropy   \n",
       "1          32     relu      0         2     relu  categorical_crossentropy   \n",
       "2          32     relu      0         2     relu  categorical_crossentropy   \n",
       "3          32     relu      0         2     relu  categorical_crossentropy   \n",
       "4          32     relu      0         2     relu  categorical_crossentropy   \n",
       "5          32     relu      0         2     relu        mean_squared_error   \n",
       "6          32     relu      0         2     relu        mean_squared_error   \n",
       "7          32     relu      0         2     relu        mean_squared_error   \n",
       "8          32     relu      0         2     relu        mean_squared_error   \n",
       "9          32     relu      0         2     relu        mean_squared_error   \n",
       "10         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "11         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "12         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "13         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "14         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "15         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "16         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "17         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "18         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "19         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "20         32     relu      0         2     tanh  categorical_crossentropy   \n",
       "21         32     relu      0         2     tanh  categorical_crossentropy   \n",
       "22         32     relu      0         2     tanh  categorical_crossentropy   \n",
       "23         32     relu      0         2     tanh  categorical_crossentropy   \n",
       "24         32     relu      0         2     tanh  categorical_crossentropy   \n",
       "25         32     relu      0         2     tanh        mean_squared_error   \n",
       "26         32     relu      0         2     tanh        mean_squared_error   \n",
       "27         32     relu      0         2     tanh        mean_squared_error   \n",
       "28         32     relu      0         2     tanh        mean_squared_error   \n",
       "29         32     relu      0         2     tanh        mean_squared_error   \n",
       "..        ...      ...    ...       ...      ...                       ...   \n",
       "780        32     tanh      0         2     relu  categorical_crossentropy   \n",
       "781        32     tanh      0         2     relu  categorical_crossentropy   \n",
       "782        32     tanh      0         2     relu  categorical_crossentropy   \n",
       "783        32     tanh      0         2     relu  categorical_crossentropy   \n",
       "784        32     tanh      0         2     relu  categorical_crossentropy   \n",
       "785        32     tanh      0         2     relu        mean_squared_error   \n",
       "786        32     tanh      0         2     relu        mean_squared_error   \n",
       "787        32     tanh      0         2     relu        mean_squared_error   \n",
       "788        32     tanh      0         2     relu        mean_squared_error   \n",
       "789        32     tanh      0         2     relu        mean_squared_error   \n",
       "790        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "791        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "792        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "793        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "794        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "795        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "796        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "797        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "798        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "799        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "800        32     tanh      0         2     tanh  categorical_crossentropy   \n",
       "801        32     tanh      0         2     tanh  categorical_crossentropy   \n",
       "802        32     tanh      0         2     tanh  categorical_crossentropy   \n",
       "803        32     tanh      0         2     tanh  categorical_crossentropy   \n",
       "804        32     tanh      0         2     tanh  categorical_crossentropy   \n",
       "805        32     tanh      0         2     tanh        mean_squared_error   \n",
       "806        32     tanh      0         2     tanh        mean_squared_error   \n",
       "807        32     tanh      0         2     tanh        mean_squared_error   \n",
       "808        32     tanh      0         2     tanh        mean_squared_error   \n",
       "809        32     tanh      0         2     tanh        mean_squared_error   \n",
       "\n",
       "    optimizer  test_accur  train_accur  \n",
       "0     rmsprop        0.56     0.700000  \n",
       "1        adam        0.56     0.700000  \n",
       "2     Adagrad        0.60     0.700000  \n",
       "3    Adadelta        0.56     0.700000  \n",
       "4         SGD        0.56     0.700000  \n",
       "5     rmsprop        0.60     0.714286  \n",
       "6        adam        0.62     0.751429  \n",
       "7     Adagrad        0.68     0.737143  \n",
       "8    Adadelta        0.44     0.300000  \n",
       "9         SGD        0.56     0.700000  \n",
       "10    rmsprop        0.60     0.720000  \n",
       "11       adam        0.60     0.717143  \n",
       "12    Adagrad        0.68     0.734286  \n",
       "13   Adadelta        0.68     0.714286  \n",
       "14        SGD        0.56     0.700000  \n",
       "15    rmsprop        0.68     0.722857  \n",
       "16       adam        0.64     0.742857  \n",
       "17    Adagrad        0.68     0.725714  \n",
       "18   Adadelta        0.68     0.714286  \n",
       "19        SGD        0.56     0.700000  \n",
       "20    rmsprop        0.56     0.405714  \n",
       "21       adam        0.54     0.542857  \n",
       "22    Adagrad        0.60     0.714286  \n",
       "23   Adadelta        0.36     0.282857  \n",
       "24        SGD        0.36     0.288571  \n",
       "25    rmsprop        0.68     0.731429  \n",
       "26       adam        0.62     0.731429  \n",
       "27    Adagrad        0.68     0.714286  \n",
       "28   Adadelta        0.66     0.702857  \n",
       "29        SGD        0.56     0.700000  \n",
       "..        ...         ...          ...  \n",
       "780   rmsprop        0.56     0.700000  \n",
       "781      adam        0.56     0.700000  \n",
       "782   Adagrad        0.56     0.700000  \n",
       "783  Adadelta        0.56     0.700000  \n",
       "784       SGD        0.56     0.700000  \n",
       "785   rmsprop        0.68     0.720000  \n",
       "786      adam        0.46     0.305714  \n",
       "787   Adagrad        0.68     0.725714  \n",
       "788  Adadelta        0.68     0.711429  \n",
       "789       SGD        0.52     0.474286  \n",
       "790   rmsprop        0.60     0.722857  \n",
       "791      adam        0.62     0.725714  \n",
       "792   Adagrad        0.68     0.725714  \n",
       "793  Adadelta        0.68     0.711429  \n",
       "794       SGD        0.56     0.700000  \n",
       "795   rmsprop        0.60     0.711429  \n",
       "796      adam        0.62     0.720000  \n",
       "797   Adagrad        0.66     0.720000  \n",
       "798  Adadelta        0.64     0.711429  \n",
       "799       SGD        0.56     0.700000  \n",
       "800   rmsprop        0.56     0.700000  \n",
       "801      adam        0.44     0.300000  \n",
       "802   Adagrad        0.56     0.700000  \n",
       "803  Adadelta        0.44     0.300000  \n",
       "804       SGD        0.56     0.700000  \n",
       "805   rmsprop        0.62     0.697143  \n",
       "806      adam        0.68     0.717143  \n",
       "807   Adagrad        0.68     0.714286  \n",
       "808  Adadelta        0.64     0.697143  \n",
       "809       SGD        0.62     0.705714  \n",
       "\n",
       "[810 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del archivo csv con los datos guardados de las corridas\n",
    "df_layact_NDrop = pd.read_csv('https://raw.githubusercontent.com/rasalav/Tarea4/master/Tarea4_ANN1.csv', index_col=0)\n",
    "df_layact_NDrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo valor de Testing obtenido para los Hiperparámetros evaluados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799999976158142"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_NDrop['test_accur'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros empleados para el máximo valor de Testing obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.734286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.708571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.734286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.731429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.728571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.725714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.717143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "7       350    350       128     relu      0        64     relu      0   \n",
       "12      350    350       128     relu      0        64     relu      0   \n",
       "13      350    350       128     relu      0        64     relu      0   \n",
       "15      350    350       128     relu      0        64     relu      0   \n",
       "17      350    350       128     relu      0        64     relu      0   \n",
       "18      350    350       128     relu      0        64     relu      0   \n",
       "25      350    350       128     relu      0        64     relu      0   \n",
       "27      350    350       128     relu      0        64     relu      0   \n",
       "30      350    350       128     relu      0        64     relu      0   \n",
       "45      350    350       128     relu      0        64     relu      0   \n",
       "56      350    350       128     relu      0        64     relu      0   \n",
       "57      350    350       128     relu      0        64     relu      0   \n",
       "65      350    350       128     relu      0        64     relu      0   \n",
       "68      350    350       128     relu      0        64     relu      0   \n",
       "72      350    350       128     relu      0        64     relu      0   \n",
       "73      350    350       128     relu      0        64     relu      0   \n",
       "75      350    350       128     relu      0        64     relu      0   \n",
       "77      350    350       128     relu      0        64     relu      0   \n",
       "86      350    350       128     relu      0        64     relu      0   \n",
       "87      350    350       128     relu      0        64     relu      0   \n",
       "90      350    350       128     relu      0        64  sigmoid      0   \n",
       "102     350    350       128     relu      0        64  sigmoid      0   \n",
       "107     350    350       128     relu      0        64  sigmoid      0   \n",
       "110     350    350       128     relu      0        64  sigmoid      0   \n",
       "115     350    350       128     relu      0        64  sigmoid      0   \n",
       "127     350    350       128     relu      0        64  sigmoid      0   \n",
       "130     350    350       128     relu      0        64  sigmoid      0   \n",
       "135     350    350       128     relu      0        64  sigmoid      0   \n",
       "156     350    350       128     relu      0        64  sigmoid      0   \n",
       "160     350    350       128     relu      0        64  sigmoid      0   \n",
       "..      ...    ...       ...      ...    ...       ...      ...    ...   \n",
       "660     350    350       128     tanh      0        64  sigmoid      0   \n",
       "670     350    350       128     tanh      0        64  sigmoid      0   \n",
       "671     350    350       128     tanh      0        64  sigmoid      0   \n",
       "672     350    350       128     tanh      0        64  sigmoid      0   \n",
       "676     350    350       128     tanh      0        64  sigmoid      0   \n",
       "677     350    350       128     tanh      0        64  sigmoid      0   \n",
       "685     350    350       128     tanh      0        64  sigmoid      0   \n",
       "686     350    350       128     tanh      0        64  sigmoid      0   \n",
       "687     350    350       128     tanh      0        64  sigmoid      0   \n",
       "700     350    350       128     tanh      0        64  sigmoid      0   \n",
       "701     350    350       128     tanh      0        64  sigmoid      0   \n",
       "705     350    350       128     tanh      0        64  sigmoid      0   \n",
       "706     350    350       128     tanh      0        64  sigmoid      0   \n",
       "717     350    350       128     tanh      0        64  sigmoid      0   \n",
       "726     350    350       128     tanh      0        64     tanh      0   \n",
       "728     350    350       128     tanh      0        64     tanh      0   \n",
       "732     350    350       128     tanh      0        64     tanh      0   \n",
       "733     350    350       128     tanh      0        64     tanh      0   \n",
       "746     350    350       128     tanh      0        64     tanh      0   \n",
       "765     350    350       128     tanh      0        64     tanh      0   \n",
       "775     350    350       128     tanh      0        64     tanh      0   \n",
       "776     350    350       128     tanh      0        64     tanh      0   \n",
       "777     350    350       128     tanh      0        64     tanh      0   \n",
       "785     350    350       128     tanh      0        64     tanh      0   \n",
       "787     350    350       128     tanh      0        64     tanh      0   \n",
       "788     350    350       128     tanh      0        64     tanh      0   \n",
       "792     350    350       128     tanh      0        64     tanh      0   \n",
       "793     350    350       128     tanh      0        64     tanh      0   \n",
       "806     350    350       128     tanh      0        64     tanh      0   \n",
       "807     350    350       128     tanh      0        64     tanh      0   \n",
       "\n",
       "     lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "7          32     relu      0         2     relu        mean_squared_error   \n",
       "12         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "13         32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "15         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "17         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "18         32     relu      0         2  sigmoid        mean_squared_error   \n",
       "25         32     relu      0         2     tanh        mean_squared_error   \n",
       "27         32     relu      0         2     tanh        mean_squared_error   \n",
       "30         32  sigmoid      0         2     relu  categorical_crossentropy   \n",
       "45         32  sigmoid      0         2  sigmoid        mean_squared_error   \n",
       "56         32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "57         32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "65         32     tanh      0         2     relu        mean_squared_error   \n",
       "68         32     tanh      0         2     relu        mean_squared_error   \n",
       "72         32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "73         32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "75         32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "77         32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "86         32     tanh      0         2     tanh        mean_squared_error   \n",
       "87         32     tanh      0         2     tanh        mean_squared_error   \n",
       "90         32     relu      0         2     relu  categorical_crossentropy   \n",
       "102        32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "107        32     relu      0         2  sigmoid        mean_squared_error   \n",
       "110        32     relu      0         2     tanh  categorical_crossentropy   \n",
       "115        32     relu      0         2     tanh        mean_squared_error   \n",
       "127        32  sigmoid      0         2     relu        mean_squared_error   \n",
       "130        32  sigmoid      0         2  sigmoid  categorical_crossentropy   \n",
       "135        32  sigmoid      0         2  sigmoid        mean_squared_error   \n",
       "156        32     tanh      0         2     relu        mean_squared_error   \n",
       "160        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "..        ...      ...    ...       ...      ...                       ...   \n",
       "660        32  sigmoid      0         2     relu  categorical_crossentropy   \n",
       "670        32  sigmoid      0         2  sigmoid  categorical_crossentropy   \n",
       "671        32  sigmoid      0         2  sigmoid  categorical_crossentropy   \n",
       "672        32  sigmoid      0         2  sigmoid  categorical_crossentropy   \n",
       "676        32  sigmoid      0         2  sigmoid        mean_squared_error   \n",
       "677        32  sigmoid      0         2  sigmoid        mean_squared_error   \n",
       "685        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "686        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "687        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "700        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "701        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "705        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "706        32     tanh      0         2  sigmoid        mean_squared_error   \n",
       "717        32     tanh      0         2     tanh        mean_squared_error   \n",
       "726        32     relu      0         2     relu        mean_squared_error   \n",
       "728        32     relu      0         2     relu        mean_squared_error   \n",
       "732        32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "733        32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "746        32     relu      0         2     tanh        mean_squared_error   \n",
       "765        32  sigmoid      0         2  sigmoid        mean_squared_error   \n",
       "775        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "776        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "777        32  sigmoid      0         2     tanh        mean_squared_error   \n",
       "785        32     tanh      0         2     relu        mean_squared_error   \n",
       "787        32     tanh      0         2     relu        mean_squared_error   \n",
       "788        32     tanh      0         2     relu        mean_squared_error   \n",
       "792        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "793        32     tanh      0         2  sigmoid  categorical_crossentropy   \n",
       "806        32     tanh      0         2     tanh        mean_squared_error   \n",
       "807        32     tanh      0         2     tanh        mean_squared_error   \n",
       "\n",
       "    optimizer  test_accur  train_accur  \n",
       "7     Adagrad        0.68     0.737143  \n",
       "12    Adagrad        0.68     0.734286  \n",
       "13   Adadelta        0.68     0.714286  \n",
       "15    rmsprop        0.68     0.722857  \n",
       "17    Adagrad        0.68     0.725714  \n",
       "18   Adadelta        0.68     0.714286  \n",
       "25    rmsprop        0.68     0.731429  \n",
       "27    Adagrad        0.68     0.714286  \n",
       "30    rmsprop        0.68     0.720000  \n",
       "45    rmsprop        0.68     0.725714  \n",
       "56       adam        0.68     0.720000  \n",
       "57    Adagrad        0.68     0.728571  \n",
       "65    rmsprop        0.68     0.737143  \n",
       "68   Adadelta        0.68     0.722857  \n",
       "72    Adagrad        0.68     0.722857  \n",
       "73   Adadelta        0.68     0.708571  \n",
       "75    rmsprop        0.68     0.734286  \n",
       "77    Adagrad        0.68     0.728571  \n",
       "86       adam        0.68     0.731429  \n",
       "87    Adagrad        0.68     0.722857  \n",
       "90    rmsprop        0.68     0.714286  \n",
       "102   Adagrad        0.68     0.717143  \n",
       "107   Adagrad        0.68     0.720000  \n",
       "110   rmsprop        0.68     0.711429  \n",
       "115   rmsprop        0.68     0.714286  \n",
       "127   Adagrad        0.68     0.714286  \n",
       "130   rmsprop        0.68     0.711429  \n",
       "135   rmsprop        0.68     0.714286  \n",
       "156      adam        0.68     0.717143  \n",
       "160   rmsprop        0.68     0.714286  \n",
       "..        ...         ...          ...  \n",
       "660   rmsprop        0.68     0.714286  \n",
       "670   rmsprop        0.68     0.714286  \n",
       "671      adam        0.68     0.720000  \n",
       "672   Adagrad        0.68     0.714286  \n",
       "676      adam        0.68     0.722857  \n",
       "677   Adagrad        0.68     0.717143  \n",
       "685   rmsprop        0.68     0.711429  \n",
       "686      adam        0.68     0.717143  \n",
       "687   Adagrad        0.68     0.714286  \n",
       "700   rmsprop        0.68     0.714286  \n",
       "701      adam        0.68     0.720000  \n",
       "705   rmsprop        0.68     0.714286  \n",
       "706      adam        0.68     0.717143  \n",
       "717   Adagrad        0.68     0.717143  \n",
       "726      adam        0.68     0.731429  \n",
       "728  Adadelta        0.68     0.714286  \n",
       "732   Adagrad        0.68     0.720000  \n",
       "733  Adadelta        0.68     0.714286  \n",
       "746      adam        0.68     0.728571  \n",
       "765   rmsprop        0.68     0.720000  \n",
       "775   rmsprop        0.68     0.717143  \n",
       "776      adam        0.68     0.722857  \n",
       "777   Adagrad        0.68     0.711429  \n",
       "785   rmsprop        0.68     0.720000  \n",
       "787   Adagrad        0.68     0.725714  \n",
       "788  Adadelta        0.68     0.711429  \n",
       "792   Adagrad        0.68     0.725714  \n",
       "793  Adadelta        0.68     0.711429  \n",
       "806      adam        0.68     0.717143  \n",
       "807   Adagrad        0.68     0.714286  \n",
       "\n",
       "[104 rows x 17 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_NDrop[df_layact_NDrop['test_accur'] == df_layact_NDrop['test_accur'].max()]\n",
    "#df_layact_NDrop.index[df_layact_NDrop['test_accur'] == df_layact_NDrop['test_accur'].max()].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo valor de Training para ese valor de Testing máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371428591864448"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_NDrop['train_accur'][df_layact_NDrop['test_accur'] == df_layact_NDrop['test_accur'].max()].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros evaluados donde se obtuvo el valor máximo de Training para el mayor Testing Accuracy logrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.737143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "7       350    350       128     relu      0        64     relu      0   \n",
       "192     350    350       128     relu      0        64     tanh      0   \n",
       "\n",
       "     lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "7          32     relu      0         2     relu        mean_squared_error   \n",
       "192        32     relu      0         2  sigmoid  categorical_crossentropy   \n",
       "\n",
       "    optimizer  test_accur  train_accur  \n",
       "7     Adagrad        0.68     0.737143  \n",
       "192   Adagrad        0.66     0.737143  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_NDrop[df_layact_NDrop['train_accur'] == df_layact_NDrop['train_accur'][df_layact_NDrop['test_accur'] == df_layact_NDrop['test_accur'].max()].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto, los parámetros de mayor testing y trainig logrados fueron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs                        350\n",
       "batch                         350\n",
       "lay1_siz                      128\n",
       "lay1_act                     relu\n",
       "Drop1                           0\n",
       "lay2_siz                       64\n",
       "lay2_act                     relu\n",
       "Drop2                           0\n",
       "lay3_siz                       32\n",
       "lay3_act                     relu\n",
       "Drop3                           0\n",
       "lout_siz                        2\n",
       "lout_act                     relu\n",
       "loss           mean_squared_error\n",
       "optimizer                 Adagrad\n",
       "test_accur                   0.68\n",
       "train_accur              0.737143\n",
       "Name: 7, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_NDrop.loc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De estos datos, se aprecia que el mejor valor obtenido para: **epochs = 350** , es **batch_size = 350**. Para estos valores,\n",
    "\n",
    "**testing_accuracy = 0.68**\n",
    "\n",
    "**trainning_acurracy = 0.737143**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la arquitectura de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,298\n",
      "Trainable params: 11,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=6))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0)) # now using dropout\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0)) # now using dropout\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0)) # now using dropout\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer='Adagrad', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229688d0588>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "#model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=0)\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=350,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 312us/step\n",
      "\n",
      " Training Accuracy: 0.7200000020435878\n",
      "50/50 [==============================] - 0s 317us/step\n",
      "\n",
      " Testing Accuracy: 0.6799999976158142\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Variación de  los Parámetro Dropout = 0 (para las tres capas del modelo)\n",
    "\n",
    "Se procedió a efectuar corridas con los siguientes parámetros:\n",
    "\n",
    "batch_size = 375, epochs = 500.\n",
    "\n",
    "Adicionalmente se procedió a variar los parámetros:layer_activ para las tres capas, Dropout, loss, optimizer; combinando valores entre ellas. Los valores de layer_activ para las tres capas, loss, optimizer se tomaron de acuerdo a los resultados de mejores testing y training del paso anterior.\n",
    "\n",
    "Se utilizaron los valores:\n",
    "\n",
    "**layer_activ = ['relu', 'sigmoid', 'tanh']**\n",
    "\n",
    "**loss = ['categorical_crossentropy', 'mean_squared_error']**\n",
    "\n",
    "**Optimizer = ['rmsprop', 'adam', 'Adagrad']**\n",
    "\n",
    "**Dropout = [0.3, 0.5, 0.8]**\n",
    "\n",
    "\n",
    "\n",
    "Los resultados encontrados fueron los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.291429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.525714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "0       500    375       128     relu    0.3        64     relu    0.3   \n",
       "1       500    375       128     relu    0.3        64     relu    0.3   \n",
       "2       500    375       128     relu    0.3        64     relu    0.3   \n",
       "3       500    375       128     relu    0.3        64     relu    0.5   \n",
       "4       500    375       128     relu    0.3        64     relu    0.5   \n",
       "5       500    375       128     relu    0.3        64     relu    0.5   \n",
       "6       500    375       128     relu    0.3        64     relu    0.8   \n",
       "7       500    375       128     relu    0.3        64     relu    0.8   \n",
       "8       500    375       128     relu    0.3        64     relu    0.8   \n",
       "9       500    375       128     relu    0.5        64     relu    0.3   \n",
       "10      500    375       128     relu    0.5        64     relu    0.3   \n",
       "11      500    375       128     relu    0.5        64     relu    0.3   \n",
       "12      500    375       128     relu    0.5        64     relu    0.5   \n",
       "13      500    375       128     relu    0.5        64     relu    0.5   \n",
       "14      500    375       128     relu    0.5        64     relu    0.5   \n",
       "15      500    375       128     relu    0.5        64     relu    0.8   \n",
       "16      500    375       128     relu    0.5        64     relu    0.8   \n",
       "17      500    375       128     relu    0.5        64     relu    0.8   \n",
       "18      500    375       128     relu    0.8        64     relu    0.3   \n",
       "19      500    375       128     relu    0.8        64     relu    0.3   \n",
       "20      500    375       128     relu    0.8        64     relu    0.3   \n",
       "21      500    375       128     relu    0.8        64     relu    0.5   \n",
       "22      500    375       128     relu    0.8        64     relu    0.5   \n",
       "23      500    375       128     relu    0.8        64     relu    0.5   \n",
       "24      500    375       128     relu    0.8        64     relu    0.8   \n",
       "25      500    375       128     relu    0.8        64     relu    0.8   \n",
       "26      500    375       128     relu    0.8        64     relu    0.8   \n",
       "27      500    375       128     relu    0.3        64     relu    0.3   \n",
       "28      500    375       128     relu    0.3        64     relu    0.3   \n",
       "29      500    375       128     relu    0.3        64     relu    0.3   \n",
       "..      ...    ...       ...      ...    ...       ...      ...    ...   \n",
       "849     500    375       128     relu    0.5        64     relu    0.5   \n",
       "850     500    375       128     relu    0.5        64     relu    0.5   \n",
       "851     500    375       128     relu    0.5        64     relu    0.5   \n",
       "852     500    375       128     relu    0.5        64     relu    0.8   \n",
       "853     500    375       128     relu    0.5        64     relu    0.8   \n",
       "854     500    375       128     relu    0.5        64     relu    0.8   \n",
       "855     500    375       128     relu    0.8        64     relu    0.3   \n",
       "856     500    375       128     relu    0.8        64     relu    0.3   \n",
       "857     500    375       128     relu    0.8        64     relu    0.3   \n",
       "858     500    375       128     relu    0.8        64     relu    0.5   \n",
       "859     500    375       128     relu    0.8        64     relu    0.5   \n",
       "860     500    375       128     relu    0.8        64     relu    0.5   \n",
       "861     500    375       128     relu    0.8        64     relu    0.8   \n",
       "862     500    375       128     relu    0.8        64     relu    0.8   \n",
       "863     500    375       128     relu    0.8        64     relu    0.8   \n",
       "864     500    375       128     relu    0.3        64     relu    0.3   \n",
       "865     500    375       128     relu    0.3        64     relu    0.3   \n",
       "866     500    375       128     relu    0.3        64     relu    0.3   \n",
       "867     500    375       128     relu    0.3        64     relu    0.5   \n",
       "868     500    375       128     relu    0.3        64     relu    0.5   \n",
       "869     500    375       128     relu    0.3        64     relu    0.5   \n",
       "870     500    375       128     relu    0.3        64     relu    0.8   \n",
       "871     500    375       128     relu    0.3        64     relu    0.8   \n",
       "872     500    375       128     relu    0.3        64     relu    0.8   \n",
       "873     500    375       128     relu    0.5        64     relu    0.3   \n",
       "874     500    375       128     relu    0.5        64     relu    0.3   \n",
       "875     500    375       128     relu    0.5        64     relu    0.3   \n",
       "876     500    375       128     relu    0.5        64     relu    0.5   \n",
       "877     500    375       128     relu    0.5        64     relu    0.5   \n",
       "878     500    375       128     relu    0.5        64     relu    0.5   \n",
       "\n",
       "     lay3_siz lay3_act  Drop3  lout_siz lout_act                      loss  \\\n",
       "0          32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "1          32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "2          32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "3          32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "4          32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "5          32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "6          32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "7          32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "8          32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "9          32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "10         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "11         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "12         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "13         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "14         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "15         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "16         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "17         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "18         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "19         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "20         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "21         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "22         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "23         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "24         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "25         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "26         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "27         32     relu    0.3         2     relu  categorical_crossentropy   \n",
       "28         32     relu    0.5         2     relu  categorical_crossentropy   \n",
       "29         32     relu    0.8         2     relu  categorical_crossentropy   \n",
       "..        ...      ...    ...       ...      ...                       ...   \n",
       "849        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "850        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "851        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "852        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "853        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "854        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "855        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "856        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "857        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "858        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "859        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "860        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "861        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "862        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "863        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "864        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "865        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "866        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "867        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "868        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "869        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "870        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "871        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "872        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "873        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "874        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "875        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "876        32  sigmoid    0.3         2     tanh  categorical_crossentropy   \n",
       "877        32  sigmoid    0.5         2     tanh  categorical_crossentropy   \n",
       "878        32  sigmoid    0.8         2     tanh  categorical_crossentropy   \n",
       "\n",
       "    optimizer  test_accur  train_accur  \n",
       "0     rmsprop        0.56     0.700000  \n",
       "1     rmsprop        0.56     0.700000  \n",
       "2     rmsprop        0.56     0.700000  \n",
       "3     rmsprop        0.56     0.700000  \n",
       "4     rmsprop        0.56     0.700000  \n",
       "5     rmsprop        0.56     0.700000  \n",
       "6     rmsprop        0.56     0.700000  \n",
       "7     rmsprop        0.56     0.700000  \n",
       "8     rmsprop        0.56     0.700000  \n",
       "9     rmsprop        0.56     0.700000  \n",
       "10    rmsprop        0.56     0.700000  \n",
       "11    rmsprop        0.56     0.700000  \n",
       "12    rmsprop        0.56     0.700000  \n",
       "13    rmsprop        0.56     0.700000  \n",
       "14    rmsprop        0.56     0.700000  \n",
       "15    rmsprop        0.56     0.700000  \n",
       "16    rmsprop        0.56     0.700000  \n",
       "17    rmsprop        0.56     0.700000  \n",
       "18    rmsprop        0.56     0.700000  \n",
       "19    rmsprop        0.56     0.700000  \n",
       "20    rmsprop        0.56     0.700000  \n",
       "21    rmsprop        0.56     0.700000  \n",
       "22    rmsprop        0.56     0.700000  \n",
       "23    rmsprop        0.56     0.700000  \n",
       "24    rmsprop        0.56     0.700000  \n",
       "25    rmsprop        0.56     0.700000  \n",
       "26    rmsprop        0.56     0.700000  \n",
       "27       adam        0.56     0.700000  \n",
       "28       adam        0.56     0.700000  \n",
       "29       adam        0.56     0.700000  \n",
       "..        ...         ...          ...  \n",
       "849      adam        0.44     0.300000  \n",
       "850      adam        0.56     0.700000  \n",
       "851      adam        0.56     0.700000  \n",
       "852      adam        0.44     0.300000  \n",
       "853      adam        0.44     0.300000  \n",
       "854      adam        0.46     0.291429  \n",
       "855      adam        0.56     0.700000  \n",
       "856      adam        0.44     0.300000  \n",
       "857      adam        0.48     0.525714  \n",
       "858      adam        0.56     0.700000  \n",
       "859      adam        0.44     0.300000  \n",
       "860      adam        0.56     0.700000  \n",
       "861      adam        0.44     0.300000  \n",
       "862      adam        0.44     0.300000  \n",
       "863      adam        0.56     0.700000  \n",
       "864   Adagrad        0.56     0.700000  \n",
       "865   Adagrad        0.56     0.700000  \n",
       "866   Adagrad        0.44     0.300000  \n",
       "867   Adagrad        0.56     0.700000  \n",
       "868   Adagrad        0.56     0.700000  \n",
       "869   Adagrad        0.56     0.700000  \n",
       "870   Adagrad        0.44     0.300000  \n",
       "871   Adagrad        0.44     0.300000  \n",
       "872   Adagrad        0.56     0.700000  \n",
       "873   Adagrad        0.44     0.300000  \n",
       "874   Adagrad        0.44     0.300000  \n",
       "875   Adagrad        0.44     0.300000  \n",
       "876   Adagrad        0.44     0.300000  \n",
       "877   Adagrad        0.56     0.700000  \n",
       "878   Adagrad        0.44     0.300000  \n",
       "\n",
       "[879 rows x 17 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del archivo csv con los datos guardados de las corridas\n",
    "df_layact_Drop = pd.read_csv('https://raw.githubusercontent.com/rasalav/Tarea4/master/Tarea4_ANN2.csv', index_col=0)\n",
    "df_layact_Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo valor de Training para ese valor de Testing máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7228571425165449"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_Drop['train_accur'][df_layact_Drop['test_accur'] == df_layact_Drop['test_accur'].max()].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros evaluados donde se obtuvo el valor máximo de Training para el mayor Testing Accuracy logrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch</th>\n",
       "      <th>lay1_siz</th>\n",
       "      <th>lay1_act</th>\n",
       "      <th>Drop1</th>\n",
       "      <th>lay2_siz</th>\n",
       "      <th>lay2_act</th>\n",
       "      <th>Drop2</th>\n",
       "      <th>lay3_siz</th>\n",
       "      <th>lay3_act</th>\n",
       "      <th>Drop3</th>\n",
       "      <th>lout_siz</th>\n",
       "      <th>lout_act</th>\n",
       "      <th>loss</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>test_accur</th>\n",
       "      <th>train_accur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "      <td>128</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.3</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.722857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epochs  batch  lay1_siz lay1_act  Drop1  lay2_siz lay2_act  Drop2  \\\n",
       "82      500    375       128     relu    0.3        64     relu    0.3   \n",
       "108     500    375       128     relu    0.3        64     relu    0.3   \n",
       "117     500    375       128     relu    0.5        64     relu    0.3   \n",
       "301     500    375       128     relu    0.3        64     relu    0.5   \n",
       "406     500    375       128     relu    0.3        64     relu    0.3   \n",
       "409     500    375       128     relu    0.3        64     relu    0.5   \n",
       "\n",
       "     lay3_siz lay3_act  Drop3  lout_siz lout_act                loss  \\\n",
       "82         32     relu    0.5         2     relu  mean_squared_error   \n",
       "108        32     relu    0.3         2     relu  mean_squared_error   \n",
       "117        32     relu    0.3         2     relu  mean_squared_error   \n",
       "301        32     relu    0.5         2  sigmoid  mean_squared_error   \n",
       "406        32     relu    0.5         2     tanh  mean_squared_error   \n",
       "409        32     relu    0.5         2     tanh  mean_squared_error   \n",
       "\n",
       "    optimizer  test_accur  train_accur  \n",
       "82    rmsprop        0.68     0.722857  \n",
       "108      adam        0.66     0.722857  \n",
       "117      adam        0.68     0.722857  \n",
       "301   Adagrad        0.66     0.722857  \n",
       "406   rmsprop        0.66     0.722857  \n",
       "409   rmsprop        0.68     0.722857  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layact_Drop[df_layact_Drop['train_accur'] == df_layact_Drop['train_accur'][df_layact_Drop['test_accur'] == df_layact_Drop['test_accur'].max()].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De estos datos, se aprecia que el mejor valor obtenido para: **epochs = 500** , es **batch_size = 375**. Para estos valores,\n",
    "\n",
    "**testing_accuracy = 0.68**\n",
    "\n",
    "**trainning_acurracy = 0.722857**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de la arquitectura de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 128)               896       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 11,298\n",
      "Trainable params: 11,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=6))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3)) # now using dropout\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) # now using dropout\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) # now using dropout\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22976c9ab70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "#model.fit(X_train, y_train, epochs=200, batch_size=100, verbose=0)\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=375,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 475us/step\n",
      "\n",
      " Training Accuracy: 0.7142857139451163\n",
      "50/50 [==============================] - 0s 80us/step\n",
      "\n",
      " Testing Accuracy: 0.6799999976158142\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(X_train, y_train)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
